{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b218690e",
   "metadata": {},
   "source": [
    "# Data Scientist Interview Questions\n",
    "## Part 4 : ML : Regression Analysis\n",
    "\n",
    "This Jupyter notebook serves as a comprehensive resource for machine learning enthusiasts and those preparing for technical interviews in the fields of data science and machine learning. It encompasses essential information and detailed insights regarding regression analysis.\n",
    "\n",
    "Whether strengthening foundational knowledge or delving into specific regression concepts, this notebook provides a concise yet thorough guide. For those aspiring to excel in data science interviews, exploring this resource offers a valuable edge in understanding regression analysis within the machine learning domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d52f4d",
   "metadata": {},
   "source": [
    "### Q0- What does regression analysis mean?\n",
    "\n",
    "- It is a statistical technique used in data science and machine learning fields. \n",
    "- It aims to model the relationship between a dependent variable and one or more independent variables.\n",
    "- By modeling the relationship between inputs and output, it is easy to understand the nature and strength of the relationship and to make predictions based on that understanding.\n",
    "\n",
    "- Mainly, we use regression analysis to resolve problems and answer questions such as:\n",
    "    - How does a change in one variable (independent variable) impact another variable (dependent variable)?\n",
    "    - Can we predict the value of the dependent variable based on the values of one or more independent variables?\n",
    "- It is widely used in various fields, including economics, finance, biology, psychology, and machine learning.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d59ba",
   "metadata": {},
   "source": [
    "### Q1- Examples of well-known machine learning algorithms used to solve regression problems\n",
    "\n",
    "Here are some well-known machine learning algorithms commonly used to solve regression problems:\n",
    "\n",
    "- Linear Regression\n",
    "- Decision Trees\n",
    "- Bayesian Regression\n",
    "- Lasso Regression\n",
    "- Ridge Regression\n",
    "- Support Vector Machines (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Random Forest\n",
    "- Gradient Boosting Algorithms (e.g., XGBoost, LightGBM)\n",
    "- Neural Networks (Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509c15d",
   "metadata": {},
   "source": [
    "### Q2- What is linear regression, and how does it work?\n",
    "\n",
    "- It is the easiest and one of the most popular Machine Learning algorithms for predictive analysis\n",
    "- LR, is a statistical method to model the relationship between a dependent variable (target) and one or more independent variables (inputs).\n",
    "- Called \"linear\" because we assume the existence of linear relationship between previous variables.\n",
    "- It aims to predict continuous/real or numeric variables such as temperature, salary, quantity, price etc. using the remaining features\n",
    "- It can be classified into two main types : \n",
    "    - Simple Linear Regression : to model relationship between an independent variable (x) and a dependent variable (y).\n",
    "    - Multiple Linear Regression : involves using more than one independent variable (X) to model the relationship with the dependent variable (y).\n",
    "- It can be used for both continuous and categorical dependent variables (y) and can handle multiple independent variables. \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee10ffd",
   "metadata": {},
   "source": [
    "## Q3- How Simple Linear Regression works? \n",
    "\n",
    "- It is used to model relationship between an independent variable (x) and a dependent variable (y).\n",
    "- Example: Predicting the price of a house based on its size.\n",
    "\n",
    "<img src=\"images/lin_reg.png\" width=\"200\">   \n",
    "\n",
    "- The line of regression, is a line of best fit is plotted on a scatter plot of the data points as it is shown in the Figure below\n",
    "- The equation of this line is : $$y=w \\times x + b$$\n",
    "\n",
    "    - Where : \n",
    "        - y: dependent/response/target variable, we want to predict it or explain it.\n",
    "        - x: independent/input/predictor variable(s), it is (they are) used to predict or explain the variability of y\n",
    "        - w: regression coefficients: the parameters in the regression equation that indicate the strength and direction of the relationship between variables.\n",
    "        - b:bias term which represents patterns that do not pass through the origin\n",
    "        \n",
    "- The line is determined by finding the values of the slope (w) and intercept (b) that minimize the sum of residuals.\n",
    "- Residuals: \n",
    "    - Corresponds to the prediction error which is differences between the observed (y) and predicted values ($\\hat y$), .\n",
    "    - Formula : $e=y-\\hat y$\n",
    "    - We calculate the Sum\n",
    "- Our main goal is to find the best fit line where the error between predicted values and actual values should be minimized.\n",
    "\n",
    "*Source:https://www.javatpoint.com/linear-regression-in-machine-learning\n",
    "\n",
    "### Q4- How Multiple Linear Regression works? \n",
    "- The unique difference between Simple and Multiple Linear Regression lies in the number of independent variables used in the regression model.\n",
    "- We have multiple independent variables $x_1, x_2, ..., x_n$\n",
    "- New equation: $y=b_0+b_1 x_1 + b_2x_2+ ...+b_n x_n$\n",
    "- Where $b_0$ represents the intercept, and $b_1, b_2, ..., b_n$ represent the coefficients of the independent variables.\n",
    "- Simple linear regression involves one independent variable, while multiple linear regression involves two or more independent variables.\n",
    "- Example: Predicting the performance of a student based on their age, gender, IQ, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad929e",
   "metadata": {},
   "source": [
    "### Q5-  What assumptions should you consider before starting a linear regression analysis?\n",
    "\n",
    "Here are some important assumptions of Linear Regression: \n",
    "\n",
    "- Linear relationship between the independent and dependent variables.\n",
    "- No or little multicolinearity between the features: independent variables are not correlated with each other\n",
    "- Normal distribution of error terms: residuals (errors), are normally distributed with a mean of zero and a constant variance.\n",
    "- The residuals are independent of each other, no autocorrelations in error terms\n",
    "- The model includes all the relevant independent variables needed to accurately predict the dependent variable.\n",
    "- Homoscedasticity Assumption: it is a situation when the error term is the same for all the values of independent variables. With homoscedasticity, there should be no clear pattern distribution of data in the scatter plot\n",
    "\n",
    "**Note:**\n",
    "- Multicollinearity involves high-correlation between the independent variables.\n",
    "- In this situation, it become difficult to find the true relationship between the predictors and target variables.\n",
    "- More precisely, it is challenging to point which predictor variable has the major influence on the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b80858",
   "metadata": {},
   "source": [
    "### Q6- What are the performance metrics for Regression Analysis? \n",
    "- Several performance metrics are commonly used to evaluate the accuracy and goodness of fit of regression models.\n",
    "- Here are some common performance metrics for regression:\n",
    "    - **Mean Absolute Error (MAE)**\n",
    "    - **Mean Squared Error (MSE)**\n",
    "    - **Root Mean Squared Error (RMSE)**\n",
    "    - **Mean Absolute Percentage Error (MAPE)**\n",
    "    - **R-squared (R2)**\n",
    "- The choice of metric is related to several goals and characteristics of the regression problem to solve.\n",
    "- It is possible to use one of the above metrics next to accuracy, precision, and the ability to explain variance.\n",
    "- Considering multiple metrics is better solution to gain a comprehensive understanding about the model performance.\n",
    "- Almost, all regression tasks uses error to evaluate the model: if error is high ==> we need either to change the model or retrain it with more data.\n",
    "\n",
    "### Q7- What is Mean Absolute Error (MAE) ? \n",
    "\n",
    "- As its name indicates, it represents the average absolute difference between the predicted values and the actual values.\n",
    "- **Formula :** $$MAE = {1\\over n} {\\sum \\limits _{i=1} ^{n}|y_{i}-\\hat{y}_{i}|}$$\n",
    "\n",
    "### Q8- What is Mean Squared Error (MSE) ?\n",
    "- It represents the average squared difference between the predicted values and the actual values.\n",
    "- It penalizes larger errors more heavily than MAE.\n",
    "- **Formula:** $$MSE = {1\\over n} {\\sum \\limits _{i=1} ^{n}(y_{i}-\\hat{y}_{i})^2}$$ \n",
    "\n",
    "### Q9- What is Root Mean Squared Error (RMSE) ? \n",
    "- It represents the square root of the MSE\n",
    "- It provides a measure of the average magnitude of errors in the same units as the target variable.\n",
    "- **Formula:** $$RMSE= {\\sqrt MSE} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47e29e",
   "metadata": {},
   "source": [
    "### Q10- What is Mean Absolute Percentage Error (MAPE) ? \n",
    "- It calculates the average percentage difference between the predicted and actual values.\n",
    "- It is a relative error metric\n",
    "- **Formula:** $$MAPE={1\\over n} {\\sum \\limits _{i=1} ^{n}({|y_{i}-\\hat{y}_{i}| \\over |y_{i}|})} \\times 100$$\n",
    "\n",
    "### Q11- What is R-squared (R2)\n",
    "- Known also as the coefficient of determination.\n",
    "- It corresponds to the degree to which the variance in the dependent variable (the target, y) can be explained by the independent variables (features). \n",
    "- Generally, it measures the proportion of variance explained by our regression model model via representing the correlation between true value and predicted value.\n",
    "- **Formula:** $$ R^2= 1 - {MSE \\over Var(y) }= 1- {{\\sum \\limits _{i=1} ^{n}(y_{i}-\\hat{y}_{i})^2} \\over {\\sum \\limits _{i=1} ^{n}(y_{i}-\\overline{y})^2}}$$\n",
    "- $\\overline{y}$: is the mean of the target variable.\n",
    "- MSE: Mean Square Error, captures the prediction error of a model\n",
    "- It is a relative metric where value varies between 0 and 1, the closer is to 1, the better is the fit.\n",
    "- \n",
    "\n",
    "\n",
    "Notes: \n",
    "- R² does not give any measure of bias, so you can have an overfitted (highly biased) model with a high value of R².\n",
    "- It is better to look at other metrics to get a good understanding of a model’s performance.\n",
    "- In the case of non-linear models, it is possible that R² is negative.\n",
    "- It is possible to use **Adjusted R-squared**, which provides a penalized version of R-squared that adjusts the model complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed062dd6",
   "metadata": {},
   "source": [
    "## How Decision Trees and Regression analysis ?\n",
    "### Q12- What does decision tree mean ? \n",
    "- It is a non-parametric supervised learning algorithm. \n",
    "- It has the tree structure: root node, edges (branches), internal and leaf nodes\n",
    "- It can be used to solve both Classification and Regression problems.\n",
    "- We build a tree with datasets broken up into smaller subsets while developing the decision tree\n",
    "- It can handle both categorical and numerical data \n",
    "\n",
    "<img src=\"images/tree_structure.png\" width=\"400\"> \n",
    "\n",
    "### Q13- What are the types of decision tree \n",
    "- We have three main types :\n",
    "    - **ID3:** Iterative Dichotomiser 3: splitting datasets is based on metrics like entropy and information gain. \n",
    "    - **C4.5:** it is identified as a later iteration of ID3, where it uses information gain or gain ratios to split datasets.\n",
    "    - **CART:** Classification And Regression Trees: it utilizes Gini impurity to identify the ideal attribute to split the dataset on.\n",
    "\n",
    "### Q14- What are the different menthods used in splitting datasets? \n",
    "Here is the list of methods:\n",
    "\n",
    "- **Variance:** \n",
    "    - Splitting datasets using the variance\n",
    "    - The variance indicates the spread or dispersion of the data. It measures how much the individual data points, in a dataset, is deviated from the average value. \n",
    "    - Prefered when the target variable is continous (Regression)\n",
    "    - It measures the node impurity via determining the homogeneity of continuous target variables within each node of the tree.\n",
    "    - High Variance means data points within a node are spread out (heterogeneous), Low Variance means data points within a node are similar and close to the mean.    \n",
    "    \n",
    "- **Entropy:** \n",
    "    - It is typically used for categorical target variables (Classification).\n",
    "    - It measures the impurity or disorder in a dataset to evaluate its homogeneity.\n",
    "    - It determines the best split to build an informative decision tree model.\n",
    "    - High entropy means that data is heterogeneous (mixed), Low entropy means data is homogeneous and well organized.\n",
    "    - Formula: $$Entropy =-{\\sum p_{i} log_{2}p_{i}}$$\n",
    "    \n",
    "- **Information gain :**\n",
    "    - It is criteria used to decide whether a feature should be used to split a node or not.\n",
    "    - Prefered when the target variable is categorical (classification)\n",
    "    - It corresponds to the mutual information betwee input attribute A and target variable Y.\n",
    "    - It quantifies the effectiveness of input attribute A in classifying the data by measuring the reduction in entropy or impurity achieved by splitting the data based on that attribute.\n",
    "    - Formula : $$IG=1- Entropy$$ \n",
    "   \n",
    "    \n",
    "- **Gini Impurity :**\n",
    "    - It is one method to split DT nodes prefered when the target variable is categorical (Classification)\n",
    "    - Formula : $$Gini Impurity =1−\\sum_{i=1}^{c}(p_{i})^2$$\n",
    "    - Where : $Gini=\\sum_{i=1}^{c}(p_{i})^2$, c is the number of classes and $p_{i}$ is the proportion of data points belonging to class i\n",
    "    - As its name indicates, it measures the impurity of a node\n",
    "    - Gini is the probability of correctly classifying a randomly chosen data point  if it was randomly labeled according to the distribution of labels in the node. \n",
    "    - Gini Impurity is the probability of incorrectly classifying a randomly chosen element if it was randomly labeled based on the distribution of classes in the dataset.\n",
    "    - Evaluation of Gini Impurity value :\n",
    "        - If value is low, the homogeneity of the node is high\n",
    "        - If value is high, the homogeneity of the node is low\n",
    "        - The Gini Impurity of a pure node is zero\n",
    "    - Gini Impurity is preferred to information gain because it does not contain logarithms which are computationally intensive an expensive.\n",
    "\n",
    "- **Chi-Square:**\n",
    "    - Again, it is one method to split DT nodes prefered when the target variable is categorical (Classification)\n",
    "    - Formula : $$Chi-Square_{class}=\\sqrt{(Actual-Expected_Value)^2 \\over Expected_Value}$$\n",
    "    - Where Expected_Value : the expected value for a class in a child node based on the distribution of classes in the parent node, and the Actual is the actual value for a class in a child node.\n",
    "    - The Chi-Square for one node has the next formula, c is the number of classes : $$ Chi-Square= \\sum_{c}(Chi-Square_{class})$$\n",
    "    - Evaluation of total Chi-Square value: \n",
    "        - If it is high, the differences between parent and child nodes is high ==> high homogeneity.\n",
    "        - If it is low, the differences between parent and child nodes is low ==> low homogeneity.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397a8f9",
   "metadata": {},
   "source": [
    " ### Q15- How many splits are there in a decision tree?\n",
    " - For classification problems : if we have n classes in a decision tree, the maximum splits will be $2^{(n -1)} – 1$ .\n",
    " ### Q16- What is the best method for splitting a decision tree?\n",
    " - Actually, there is no best method that is suitable for all problems but it depends on the problem itself and the target variable we want to estimate (continous or discrete)\n",
    " - The most widely used method for splitting a decision tree is the gini index or the entropy.\n",
    " - The scikit learn library provides all the splitting methods for classification and regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b9d18",
   "metadata": {},
   "source": [
    "### Q17- What are the Advantages of Decision Trees?\n",
    "\n",
    "- Easy to interpret\n",
    "- Little or no data preparation is needed: handle categorical and continous variables, handle variables with missing values.\n",
    "- High flexibility : it is used for both classification and regression tasks.\n",
    "\n",
    "### Q18- What are the disadvantages of Decision Trees ?\n",
    "\n",
    "- Prone to overfitting : building complex decision tree model can easily overfit because it does not generalize well to unseen data\n",
    "- High variance estimators :  even little variations within data can produce a very different decision tree\n",
    "- Too costly: it uses greedy search approach to build decision tree model therefore, training this model can be very expensive to train compared to other algorithms. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45cf4b",
   "metadata": {},
   "source": [
    "### Q19- How to avoid Overfitting of a Decision Tree model?\n",
    "- Overfitting corresponds to the situation when the model captures noise or random fluctuations in the training data which gives poor performance on unseen data. \n",
    "- Here a list of methods that could help in avoiding overfitin in DT:\n",
    "    - Limiting Tree Depth : avoiding having complex trees\n",
    "    - Pruning : using pre-pruning or post-pruning algorithm.\n",
    "    - Feature Selection: select the most important features and discard irrelevant and redundant features\n",
    "    - Cross-Validation: appy cross validation technique such as K-fold cross validation to better tune hyperparameters and prevent overfitting\n",
    "    - Ensemble learning algorthims: such as Random Forests or Gradient Boosting where it leverages combining multiple models to limit overfitting .\n",
    "    - Minimum Samples per Leaf\n",
    "    - Minimum Samples per Split\n",
    "    \n",
    "    \n",
    "### Q19.1 - What does Pre-pruning and Post-pruning mean?\n",
    "- Pre-pruning halts tree growth when there is insufficient data.\n",
    "- Post-pruning removes subtrees with inadequate data after tree construction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92130ed8",
   "metadata": {},
   "source": [
    "### Q20- What does prunning decision tree mean?\n",
    "\n",
    "- Pruning is a technique in ML that reduces the size of DT.\n",
    "- It aims to reduce the complexity of final classifier \n",
    "- Pruning helps improve the predictive accuracy by reducing overfitting \n",
    "- Pruning can occur in :\n",
    "    - Top-down fashion \n",
    "    - Bottom-down fashion\n",
    "- Top-down fashion : it will traverse nodes and train subsets starting at the root\n",
    "- Bottom-up fashion : it will begin at the leaf nodes \n",
    "\n",
    "### Q21- What are the popular pruning algorithms?\n",
    "- Reduced error pruning :\n",
    "    - starts with leaves, each node is replaced with its most popular class\n",
    "    - if the prediction accuracy is not affected the change is kept \n",
    "    - There is an advantage of simplicity and speed \n",
    "### Q22- What are the stopping criteria in splitting datasets in decision trees\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb16512",
   "metadata": {},
   "source": [
    "### Q23 - Final comments regarding Decision trees\n",
    "- Decision trees are very common algorithm in machine learning for solving both classification and regression problems.\n",
    "- However, creating an optimal decision tree requires finding the right features and splitting the data in an effective way that maximizes information gain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb941a",
   "metadata": {},
   "source": [
    "### What does Ensemble learning mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf3a36",
   "metadata": {},
   "source": [
    "## 5- What does Random Forest mean and how it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104b2db",
   "metadata": {},
   "source": [
    "## 6- What does Bayesian Regression mean and how it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d68d498",
   "metadata": {},
   "source": [
    "## 7- How Gradient Boosting Algorithms (e.g., XGBoost, LightGBM) are used to resolve regression problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353f543",
   "metadata": {},
   "source": [
    "## 8- How can we use Neural Network to resolve regression problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2b4a0",
   "metadata": {},
   "source": [
    "## 9- How determine whether a predictive model is underfitting or overfitting the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5753a5",
   "metadata": {},
   "source": [
    "## 10-What is Lasso Regression and how it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdcc46",
   "metadata": {},
   "source": [
    "## 11- What is Ridge Regression and how it works?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
