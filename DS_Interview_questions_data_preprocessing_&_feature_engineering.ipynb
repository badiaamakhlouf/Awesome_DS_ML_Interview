{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e33277a",
   "metadata": {},
   "source": [
    "#  Data Science Interview questions\n",
    "## Part 1 : Data cleaning, pre-processing, EDA\n",
    "This Jupyter notebook serves as a comprehensive resource for data science interview questions, focusing specifically on topics related to data cleaning, preprocessing, and analysis. Here, you'll find a curated collection of questions that span various aspects of data preparation, transformation, and exploratory data analysis. Whether you're preparing for an interview or seeking to deepen your understanding of essential data science concepts, this notebook aims to provide a structured and informative guide to help you navigate through key challenges in the field. Explore the questions, test your knowledge, and enhance your proficiency in the fundamental stages of data science workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00814acc",
   "metadata": {},
   "source": [
    "### 1- How to deal with missing values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72be9b",
   "metadata": {},
   "source": [
    "### 2- How to detect outliers in data set ? \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf459d8d",
   "metadata": {},
   "source": [
    "### 3- How to handel duplicates ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a875f2b",
   "metadata": {},
   "source": [
    "### 4- What does normal distribution means ?\n",
    "The normal distribution is very useful in machine learning becasue it has deterministic statistical characteristics  and it helps detect linear relationship between variables. It consists that mode=mean=median: \n",
    "\n",
    "- Mean: called also average of a data set and it is found by summing all numbers in the data set and then dividing by the number of values in the set.\n",
    "- Mode : it is the value that appears most often in a set of data values.\n",
    "- Median : the middle number; found by ordering all data points and picking out the one in the middle (or if there are two middle numbers, taking the mean of those two numbers).\n",
    "\n",
    "### 5- What does percentile and quantile means ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a5160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fef9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9e5f8f",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699c792",
   "metadata": {},
   "source": [
    "### 1- What does feature engineering means? \n",
    "\n",
    "Feature engineering refers to the process of raw data manipulation such as addition, deletion, combination, mutation etc. It encompasses the process of creating new features or modifying existing ones to improve the performance of a machine learning model. \n",
    "\n",
    "Here is a range of significant activities used in Feature Engineering :\n",
    "\n",
    "- Feature Selection\n",
    "- Data Transformation\n",
    "- Text Data Processing\n",
    "- Time-Series Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e238ca8",
   "metadata": {},
   "source": [
    "### 2- What does data transformation means?\n",
    "\n",
    "Data transformation is indeed one subtask within the broader field of feature engineering in machine learning. It is a specific aspect of feature engineering that involves modifying the raw data to make it more suitable for the learning algorithm.\n",
    "It includes : \n",
    "- Feature Scaling\n",
    "- Feature encoding\n",
    "- Feature extraction\n",
    "- Binning or Discretization\n",
    "- Creating Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0d63c",
   "metadata": {},
   "source": [
    "### 3- What does feature scaling mean ?\n",
    "Feature scaling is a preprocessing step in machine learning that involves transforming the numerical features of a dataset to a common scale. Feature scaling is particularly important for algorithms that rely on distance metrics or gradient descent optimization.\n",
    "\n",
    "Here are common techniques for feature scaling:\n",
    "- Normalization\n",
    "- Standard scaling : converts features to standard normal variables (by subtracting the mean and dividing the standard error)\n",
    "- Log scaling or Log transformation\n",
    "- Polynomial transformation\n",
    "- Robust scaling\n",
    "\n",
    "#### 3. 1- Why do we need perform feature scaling ? \n",
    "The goal is to ensure that all features contribute equally to the learning process and to prevent certain features from dominating due to differences in their magnitudes.\n",
    "\n",
    "#### 3. 2- Normalization - Min-Max Scaling\n",
    "- Scales the feature values to a specific range, usually between 0 and 1\n",
    "- Formula : $X_{normalized}= {X-X_{min}\\over X_{max}-X_{min}}$\n",
    "\n",
    "#### 3. 3- Standard scaling - Z-score normalization\n",
    "- Centers the feature values around zero with a standard deviation of 1.\n",
    "- Suitable for algorithms that assume a normal distribution of features.\n",
    "- Formula: $X_{standardized} ={ X - mean(X) \\over std(X)}$\n",
    "\n",
    "#### 3. 4- Robust Scaling\n",
    "- Scales the features based on the interquartile range (IQR) to handle outliers.\n",
    "- Formula: $X_{robust} = {X - median(X)\\over IQR(X)}$\n",
    "\n",
    "#### a. IQR : interquartile range\n",
    "- The IQR is the difference between the third quartile (Q3) and the first quartile (Q1): IQR = Q3 - Q1\n",
    "- Q1 : It represents the median of the lower 50% of the data.\n",
    "- Q3 : It represents the median of the upper 50% of the data\n",
    "\n",
    "Here's how you calculate the IQR: \n",
    "- 1. Order the dataset: arrange the values in the dataset in ascending order\n",
    "- 2. Determine the median (Q2): which is the middle value of the dataset. If the dataset has an odd number of observations, the median is the middle value. If it has an even number, the median is the average of the two middle values.\n",
    "- 3. Find the First Quartile (Q1)\n",
    "- 4. Find the Third Quartile (Q3)\n",
    "- 5. Calculate the IQR\n",
    "\n",
    "The IQR provides a robust measure of the spread of the middle 50% of the data, making it less sensitive to extreme values or outliers. It is commonly used in box plots to visually represent the dispersion of data.\n",
    "\n",
    "#### 3. 5- Log Transformation\n",
    "\n",
    "- The log transformation is the most popular among the different types of transformations used in machine learning.\n",
    "- It aims to make highly skewed distributions (features with high variance) less skewed.\n",
    "- The logarithm used is often the natural logarithm (base e) or the common logarithm (base 10).\n",
    "- Generally, we use the natural logarithm function in Log transformation.\n",
    "- If the original data follows a log-normal distribution or approximately so, then the log-transformed data follows a normal or near normal distribution.\n",
    "- However, our real raw data do not always follow a normal distribution. They are often so skewed making the results of our statistical analyses invalid. That’s where Log Transformation comes in.\n",
    "\n",
    "#### 3. 6- Polynomial transformation\n",
    "- It is a feature engineering technique used in machine learning and statistics to capture non-linear relationships between variables.\n",
    "- It involves transforming input features by raising them to the power of an integer, creating polynomial terms. The most common form is the quadratic transformation (squared terms), but higher-order polynomials can also be used.\n",
    "- Such transformations are often beneficial for machine learning algorithms, particularly in tasks involving numerical input variables, improving predictive accuracy, especially in regression tasks.\n",
    "- If X is one input feature ==> $X^2$ is its polynomial feature.\n",
    "- The “degree” of the polynomial is used to control the number of features added, e.g. a degree of 3 will add two new variables for each input variable. Typically a small degree is used such as 2 or 3. Choosing the best polynomial degree is so important as it impacts the number of input features created. \n",
    "\n",
    "**More notes :** \n",
    "\n",
    "- Higher-degree polynomials (Degree > 2) can lead to overfitting, capturing noise in the data rather than true underlying patterns. Regularization techniques may be needed to mitigate this.\n",
    "- It's important to scale features before applying polynomial transformations to prevent features with larger scales from dominating the transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba2d42",
   "metadata": {},
   "source": [
    "### 4- How to deal with categorical values ?\n",
    "- Drop categorical variables\n",
    "- Perform feature encoding\n",
    "\n",
    "#### 4.1- What does feature encoding means? \n",
    "\n",
    "Feature encoding is the process of converting categorical data or text data into a numerical format that can be easily used for machine learning algorithms. In many machine learning models, the input features are expected to be numerical, and encoding is necessary when dealing with non-numeric data.\n",
    "\n",
    "Here are some common encoding methods: \n",
    "- Ordinal encoding: Assign numerical values based on the inherent order of categories\n",
    "- One-hot encoding : Create binary columns for each category, indicating its presence (1) or absence (0)\n",
    "- Label Encoding : Assign a unique numerical label to each category in a categorical variable\n",
    "- Binary Encoding : Convert each category into its binary representation.\n",
    "- Frequency (Count) Encoding: Replace each category with its frequency or count in the dataset\n",
    "\n",
    "\n",
    "**!! Notes :**\n",
    "- Ordianl encoding is a good choice in case we have ranking in our categorical variables (Low, medium, high), most used with DT and Random Forest.\n",
    "- One-hot encoding is more used when there is no ranking in the categorical variables.\n",
    "- If our dataset is very large (high cardinality) --> one-hot encoding can greatly expand the size of dataset : number columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5cb2a",
   "metadata": {},
   "source": [
    "### 5- What does Feature extraction means?\n",
    "One of the primary goals of feature extraction is to reduce the dimensionality of the dataset. High-dimensional data can lead to the curse of dimensionality, making it challenging for models to generalize well.\n",
    "\n",
    "Feature extraction aims to retain the most relevant information from the original data. This involves identifying features that contribute significantly to the variability and patterns within the dataset while discarding redundant or irrelevant information.\n",
    "\n",
    "Here are all types of Feature Extraction:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Singular Value Decomposition (SVD)\n",
    "- Independent Component Analysis (ICA)\n",
    "- Bag-of-Words (BoW)\n",
    "\n",
    "#### 5. 1- What does Principal Component Analysis (PCA) means ? \n",
    "\n",
    "#### 5. 2- What does Singular Value Decomposition (SVD) means ? \n",
    "#### 5. 3- What does Independent Component Analysis means ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58ef1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43ef9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd15dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c90b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c5a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
