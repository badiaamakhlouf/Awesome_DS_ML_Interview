{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e33277a",
   "metadata": {},
   "source": [
    "#  Data Science Interview questions\n",
    "## Part 1 : Data cleaning, pre-processing, EDA\n",
    "This Jupyter notebook serves as a comprehensive resource for data science interview questions, focusing specifically on topics related to data cleaning, preprocessing, and analysis. Here, you'll find a curated collection of questions that span various aspects of data preparation, transformation, and exploratory data analysis. Whether you're preparing for an interview or seeking to deepen your understanding of essential data science concepts, this notebook aims to provide a structured and informative guide to help you navigate through key challenges in the field. Explore the questions, test your knowledge, and enhance your proficiency in the fundamental stages of data science workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970e3fc",
   "metadata": {},
   "source": [
    "### 0- What are the main tasks of data cleaning in Data Science?\n",
    "Here are the main tasks to perform in the cleaning phase :\n",
    "- Finding and handling missing data\n",
    "- Finding and handling duplicates\n",
    "- Finding and handling outliers\n",
    "\n",
    "Note : encoding categorical data can be done in feature engineering phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00814acc",
   "metadata": {},
   "source": [
    "### 1- How to deal with missing values ?\n",
    "\n",
    "Handling missing values is a crucial step in data preprocessing to ensure accurate and unbiased analysis. Here are two main methods to deal with missing values: \n",
    "- Remove missing values \n",
    "- Impute missing values \n",
    "- Forward or Backward Fill\n",
    "\n",
    "\n",
    "**Note:** to make better predictions we can add an extension to imputation.\n",
    "\n",
    "Choosing the right method is based on:\n",
    "- The characteristics of the data\n",
    "- The percentage of missing values\n",
    "- The goals of the performed analysis.\n",
    "\n",
    "No single method is suitable for all situations, so it's essential to understand the context and implications of each approach.\n",
    "\n",
    "#### 1. 1- How to detect or identify missing values? \n",
    "- Identifying missing values is the first step to perform when dealing with them. \n",
    "- Using Pandas functions like `isnull()` or `info()`.\n",
    "\n",
    "#### 1. 2- How to remove missing values? \n",
    "\n",
    "Here is how to remove missing values :\n",
    "- Remove Rows with nan/null values using `df = df.dropna()`\n",
    "- Remove Columns with nan/null values using `df = df.dropna(axis=1)`\n",
    "\n",
    "Dropping rows or columns is not too advantageous because most values are going to be lost and they contain important information\n",
    "\n",
    "#### 1. 3- How to impute missing values? \n",
    "\n",
    "We have four main methods:\n",
    "\n",
    "    - Impute with statistical measures\n",
    "    - Impute with a Placeholder \n",
    "    - Impute with Machine Learning Algorithms\n",
    "    - Impute using Interpolation\n",
    "    - Multiple Imputation\n",
    "Imputed value won't be exactly right in most cases but it usually leads to more accurate models than you would get from dropping the column entirely  \n",
    "    \n",
    "##### a. What does impute with a statistical measures mean ? \n",
    "- Fill missing values with statistical measures (mean, median, mode) or using more advanced imputation methods.\n",
    "- Example: `df['column'] = df['column'].fillna(df['column'].mean())`\n",
    "    \n",
    "##### b. What does impute with a Placeholder mean ? \n",
    "- Replace with a specific value that does not occur naturally in the dataset. \n",
    "- Example: `df = df.fillna(-1)`\n",
    "\n",
    "##### c. What does impute with a Machine Learning Algorithm mean ?    \n",
    "- **Solution 1:**  use `KNNImputer()` class from the scikit-learn Python library.\n",
    "- **Solution 2:**\n",
    "    - Train a machine learning model to predict missing values based on other features in the dataset.\n",
    "    - Example : Random Forest \n",
    "##### d. What does Impute using Interpolation mean ?\n",
    "- Interpolation is a technique used to estimate missing values based on the observed values in a dataset.\n",
    "- It works by filling in the gaps between known data points, assuming some underlying pattern or relationship.\n",
    "- Here are some interpolation techniques:\n",
    "    - Linear Interpolation \n",
    "    - Polynomial Interpolation\n",
    "    - Quadratic \n",
    "    - Etc.\n",
    "\n",
    "Note : the choice of the right interpolation method depends on:\n",
    "- The nature of the data.\n",
    "- The assumptions about its behavior\n",
    "##### e. What does multiple imputation mean ? \n",
    "- It is a statistical technique used to handle missing data via creating multiple imputed datasets. \n",
    "- Multiple datasets are created by imputing missing values using a chosen imputation method. \n",
    "- Examples : mean imputation, regression imputation, k-Nearest Neighbors imputation, or more sophisticated methods.\n",
    "- Each dataset represents a set of values for the missing entries.\n",
    "- Instead of imputing a single value for each missing observation, multiple imputation illustrates the uncertainty associated with missing data by generating several imputed datasets. \n",
    "- The results from the analyses conducted on the imputed datasets are combined, or \"pooled,\" to obtain an overall estimate of the parameter of interest.\n",
    "- The combined results provide not only a point estimate but also an estimate of the uncertainty associated with the missing data. This incorporates both the imputation variability and the variability due to analyzing different imputed datasets.\n",
    "- `fancyimpute()` Python library can be employed to implement multiple imputation efficiently.\n",
    "\n",
    "#### 1. 4- Why do we need an extension to imputation? \n",
    "\n",
    "- Sometimes, missing values themselves can be indicative. Create a new binary column indicating whether a value is missing. \n",
    "- For each column with missing entries in the original dataset, we add a new column that shows the location of imputed entries. \n",
    "- Models would make better predictions by considering which values were originally missing.   \n",
    "- Example:  `df['column_missing'] = df['column'].isnull().astype(int)` \n",
    "\n",
    "#### 1. 5- Why it is better to use the median value for imputation in the case of outliers?\n",
    "- Using the median for imputation in case of outliers is often considered a better solution compared to the mean.\n",
    "- The median is a measure of central tendency that has: \n",
    "    - **Robustness to Outliers:** it is less influenced by extreme values because it is not affected by the actual values of data points but rather their order. Outliers have a minimal impact on the median.\n",
    "    - **Resilient to Skewness:** in a skewed distribution, where the tail is longer in one direction, the mean can be heavily influenced by the skewness. The median, being the middle value, is less affected by the skewness and provides a more representative measure in such situations.\n",
    "    - **Ability to avoid Biased Estimates:** in the presence of outliers, using the mean for imputation might lead to biased estimates, especially when the distribution is not symmetric. The median provides a more balanced estimate in skewed or asymmetric distributions.\n",
    "    - **Ability to maintain Robustness in Non-Normal Distributions:** in case our data does not have a normal distribution, the median is often a more reliable measure of central tendencyas it helps in producing more accurate imputations.\n",
    "    \n",
    "#### 1. 6-  How to perform Forward or Backward Fill   ? \n",
    "Propagate the last valid observation forward or use the next valid observation to fill missing values: \n",
    "\n",
    "- Forward fill using : `df = df.ffill()`  or `df.fillna(method='ffill')`\n",
    "- Backward fill using : `df = df.bfill()` or `df.fillna(method='bfill')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf459d8d",
   "metadata": {},
   "source": [
    "### 2- How to handle duplicates ? \n",
    "Handling duplicates in data science is an essential step to ensure data quality and avoid biases or inaccuracies in analysis. Here are common methods to handle duplicates:\n",
    "- 1- Identifying Duplicates using `duplicated()` using Pandas\n",
    "- 2- Removing Duplicates - all : `df = df.drop_duplicates()`\n",
    "- 3- Removing Duplicates - Keep first Occurrences : `df = df.drop_duplicates(keep='first')`\n",
    "- 4- Removing Duplicates - Keep last Occurrences : `df = df.drop_duplicates(keep='last')`\n",
    "- 5- Handling Duplicates Based on Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c9c5e",
   "metadata": {},
   "source": [
    "### 3- How to find outliers?\n",
    "To find outliers, only numerical columns are considered in our analysis. Here are the common methods to do that :\n",
    "- Visualization technique :  Box Plot, Scatter Plot and Histogram Plot (the most used ones).\n",
    "- Mathematical approach :\n",
    "    - Z-score\n",
    "    - Interquartile range : IQR score \n",
    "- Machine Learning Models :\n",
    "    - Clustering Algorithms\n",
    "    - Isolation Forest\n",
    "- Domain-Specific Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1dd7b",
   "metadata": {},
   "source": [
    "#### 3. 1-  How to handle outliers in dataset ? \n",
    "Here are some methods about how we handle outliers :\n",
    "\n",
    "- **Deleting the values:** removing the value completely, if we are sure that this value is wrong and it will never occur again, we remove it using either Interquartile range or Z-score.\n",
    "- **Replace the values:** change the values if we know the reason for the outliers. (Example: using 99th percentile)\n",
    "- **Data transformation:** some times data transformation such as natural log reduces the variation caused by the extreme values. Most used for highly skewed data sets.\n",
    "\n",
    "#### a. What does Z-Score mean?\n",
    "- It calculates the Z-score for each data point.\n",
    "- Z-score measures how many standard deviations a data point is from the mean.\n",
    "- Typically, a threshold of 2 to 3 standard deviations is used to identify outliers.\n",
    "- Formula: $Z ={ X - \\mu \\over\\sigma}$\n",
    "\n",
    "#### b. What does IQR : interquartile range mean? \n",
    "- The IQR is the difference between the third quartile (Q3) and the first quartile (Q1): IQR = Q3 - Q1\n",
    "- Q1: It represents the median of the lower 50% of the data. Represents 0.25 percentile\n",
    "- Q3 : It represents the median of the upper 50% of the data. Represents 0.75 percentile\n",
    "\n",
    "To calculate percentiles or quantiles, we need to sort the data in ascending order and finding the value below which a certain percentage of the data falls.\n",
    "\n",
    "![title](images/boxplot.png) \n",
    "#### c. How ML Algorithms used for outliers detection ?\n",
    "We have two main methods: \n",
    "- **Clustering Algorithms:** for example k-means can be used to detect outliers where points that do not belong to any cluster or are in small clusters can be identified as outliers.\n",
    "- **Isolation Forest:** designed specifically for outlier detection. It isolates outliers by recursively partitioning the data.\n",
    "\n",
    "Notes: \n",
    "- Box plot is considered as Uni-variate analysis\n",
    "- Scatter plot is considered as Bi-variate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72be9b",
   "metadata": {},
   "source": [
    "#### 3. 2-  What does percentile and quantile mean ?\n",
    " Percentile and quantile are statistical concepts used to describe the relative standing or distribution of a particular value within a dataset. Both concepts help understand the position of a data point in relation to other values.\n",
    "#### a- Percentile\n",
    "- A percentile is a measure that indicates the relative standing of a particular value within a dataset.\n",
    "- The pth percentile is the value below which p percent of the data falls.\n",
    "- The dataset is divided into 100 equal parts\n",
    "#### b- Quantile \n",
    "- A quantile is a generic term for dividing the data into intervals or groups of equal probability.\n",
    "- Percentiles are a specific type of quantile, where the division is based on percentages.\n",
    "- The term \"quantile\" is often used more broadly to refer to any division of the data, not necessarily in increments of 1% (as in percentiles).\n",
    "- For example : **Quartile**: the dataset is divided into four equal parts (Q1, Q2, Q3):\n",
    "    - Q1: the 25th percentile is the value below which 25% of the data falls.\n",
    "    - Q2: the 50th percentile is the value below which 50% of the data falls\n",
    "    - Q3: the 75th percentile is the value below which 75% of the data falls\n",
    "#### c- Calculation : \n",
    "- The calculation of percentiles and quantiles involves sorting the data in ascending order and finding the value below which a certain percentage of the data falls.\n",
    "- **Example:** if a student scores in the 90th percentile on a standardized test, it means they performed better than 90% of the students who took the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e7c10",
   "metadata": {},
   "source": [
    "### 4- What does Exploratory Data Analysis (EDA) mean? \n",
    "It is a critical step in the data analysis process and is often the second step after cleaning the provided dataset. The primary goal of EDA is to summarize the main characteristics of a dataset, gain insights into the underlying structure, identify patterns, detect anomalies, and formulate hypotheses for further analysis.\n",
    "\n",
    "Key aspects of Exploratory Data Analysis include:\n",
    "- Summary Statistics using `.describe()` pandas library.\n",
    "- Data Visualization\n",
    "- Distribution Analysis\n",
    "- Correlation Analysis\n",
    "\n",
    "Effective EDA aims to perform more targeted and informed analyses, leading to better decision-making and valuable insights from the data.\n",
    "\n",
    "#### 4. 1- What does Distribution Analysis mean?\n",
    "- This analysis aims to examine the distribution of values within a dataset.\n",
    "- Understanding the distribution of data is essential for gaining insights into its underlying characteristics, identifying patterns, and making informed decisions about subsequent analyses or modeling.\n",
    "- Here are some examples of distribution analysis: \n",
    "    - Frequency Distribution:  It provides a summary of how often each value appears. We can use `.value_counts()` Pandas library.\n",
    "    - Univariate and Bivariate Analysis : distplot, histplot and X versus Y etc.\n",
    "    - Probability Distribution\n",
    "    - Spread or Dispersion analysis\n",
    "    - Skewness and Kurtosis analysis\n",
    "    \n",
    "- Understanding the data distribution is very important in many tasks, including identifying outliers, assessing the appropriateness of statistical models, and making decisions about data transformations.\n",
    "- Different types of distributions may require different approaches in data analysis and modeling, and distribution analysis helps inform these decisions.\n",
    "\n",
    "#### a. What does normal distribution mean ?\n",
    "The normal distribution is very useful in machine learning becasue it has deterministic statistical characteristics  and it helps detect linear relationship between variables. It consists that mode=mean=median: \n",
    "\n",
    "- Mean: called also average of a data set and it is found by summing all numbers in the data set and then dividing by the number of values in the set.\n",
    "- Mode : it is the value that appears most often in a set of data values.\n",
    "- Median : the middle number; found by ordering all data points and picking out the one in the middle (or if there are two middle numbers, taking the mean of those two numbers).\n",
    "\n",
    "#### b. What does Skewness and Kurtosis mean ?\n",
    "**Skewness:**\n",
    "- It is a measure of the asymmetry of a distribution.\n",
    "- A distribution is asymmetrical when its left and right side are not mirror images.\n",
    "- A skewed data can not be used to generate normal distribution. \n",
    "- It provides insights into the shape of a distribution.\n",
    "- The three types of skewness are:\n",
    "    - **Skewness > 0 :** right (or positive) skewness. This indicates that the tail on the right side is longer or fatter than the left side, and the majority of the data points are concentrated on the left side.\n",
    "    - **Skewness < 0 :** left (or negative) skewness. It means the tail on the left side is longer or fatter than the right side, and the majority of the data points are concentrated on the right side.\n",
    "    - **Skewness=0, Zero skewness :** the distribution is perfectly symmetrical.\n",
    "    \n",
    "<img src=\"images/Skewness.png\" width=\"400\">\n",
    "    \n",
    "**Kurtosis:**\n",
    "- A statistical measure that describes the shape or \"tailedness\" of a distribution. \n",
    "- It provides information about the concentration of data points in the tails relative to the center of the distribution. \n",
    "- The three types of Kurtosis are:\n",
    "    - **Kurtosis=0 (Mesokurtic) :** the distribution has the same tail behavior as a normal distribution.\n",
    "    - **Kurtosis>0 (Leptokurtic):** the distribution has fatter tails (heavier tails) and a sharper peak than a normal distribution. This indicates a higher probability of extreme values.\n",
    "    - **Kurtosis<0 (Platykurtic):** the distribution has thinner tails (lighter tails) and a flatter peak than a normal distribution. This suggests a lower probability of extreme values.\n",
    "   \n",
    "kurtosis measures whether the data is heavy-tailed (more extreme values than a normal distribution) or light-tailed (fewer extreme values than a normal distribution).\n",
    "\n",
    "\n",
    "<img src=\"images/Kurtosis.png\" width=\"400\">\n",
    "\n",
    "\n",
    "#### c. What does Spread or Dispersion mean ?\n",
    "- Data spread: \n",
    "    - It provides information about the range of values in a dataset.\n",
    "    - It provides information about how dispersed or scattered the individual data points are around a measure of central tendency, such as the mean or median.\n",
    "    - Spread measures help to understand the variability or dispersion of the data.\n",
    "    - **Examples: IQR, range, variance, standard deviation** \n",
    "    - It is crucial to understand the spread of data for better outliers detection, risk assessment, decision-Making etc.\n",
    "- Dispersion:\n",
    "    - It explains how individual data points in a dataset deviate or spread out from a central measure of tendency, such as the mean or median. \n",
    "    - Dispersion measures provide insights into the variability or spread of the data and are crucial for understanding the overall distribution.\n",
    "    - **Examples: IQR, range, variance, standard deviation, Mean Absolute Deviation (MAD), Coefficient of Variation (CV)**\n",
    "\n",
    "#### d. How to get Summary Statistics ? \n",
    "- In the statistical description we try to select the next values for each numerical features:\n",
    "    - Maximum values\n",
    "    - Minimum\n",
    "    - Average\n",
    "    - Standard deviation\n",
    "    - Median\n",
    "    - Mean\n",
    "- Code: `df.describe().transpose()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e04f7",
   "metadata": {},
   "source": [
    "#### 4. 1- What does Correlation Analysis mean?\n",
    "- Correlation analysis is a statistical method used to evaluate the strength and direction of the linear relationship between two quantitative variables.\n",
    "- The result of a correlation analysis is a correlation coefficient, which quantifies the degree to which changes in one variable correspond to changes in another.\n",
    "- Correlation analysis is widely used in various fields, including economics, biology, psychology, and data science, to understand relationships between variables and make predictions based on observed patterns.\n",
    "##### a. What are the plot used to illustrate correlation?\n",
    "- Correlation matrix and heatmap \n",
    "- Scatter Plot : it provides a visual representation of the relationship between two variables. X versus Y\n",
    "##### b. How to interpret Correlation Coefficient ? \n",
    "- It is a numerical measure that ranges from -1 to 1.\n",
    "- A positive correlation >0 : indicates that if one variable increases, the other variable tends to increase as well.\n",
    "- A negative correlation <0 : indicates that if one variable increases, the other variable tends to decrease.\n",
    "- A correlation coefficient equal to 0: indicates no linear relationship between the variables.\n",
    "- **Strength of Correlation:**\n",
    "    - The closer the correlation coefficient is to -1 or 1, the stronger the correlation. A coefficient of -1 or 1 implies a perfect linear relationship.\n",
    "    - A coefficient closer to 0 indicates a weaker linear relationship.\n",
    "##### c. What does correlation matrix means? \n",
    "- It is a table that displays the correlation coefficients between many variables. \n",
    "- Each cell Corresponds to the correlation coefficient between two variables. \n",
    "- This matrix helps detect the presence of any positive or negative correlation between variables.\n",
    "- The correlation is calculated using the pearson correlation coefficient so values varies from -1 to 1\n",
    "\n",
    "##### d. Pearson Correlation vs. Spearman Correlation? \n",
    "- If variables have a linear relationship and follow a normal distribution ==> then use Pearson correlation. \n",
    "- Spearman correlation is a non-parametric measure that assesses the strength and direction of monotonic relationships (whether the variables tend to increase or decrease together, but not necessarily at a constant rate)\n",
    "##### e. Cautions and Considerations when analysing correlation?\n",
    "- Correlation does not imply causation : even if two variables are correlated, it does not necessarily mean that one causes the other.\n",
    "- Outliers can have a significant impact on correlation results, so it's important to check for their presence.\n",
    "- Correlation analysis is sensitive to the scale of measurement, so data standardization(eg. z-scores) can be improve results.\n",
    "- Other data transformation such as log tranformation can improve the correlation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43085e5",
   "metadata": {},
   "source": [
    "#### 4.2- What else we can perform in EDA ? \n",
    "Here are more analysis to perform during EDA phase:\n",
    "- Data frame dimension `df.shape`\n",
    "- Data frame columns: `df.columns`\n",
    "- Count values: `df['SaleCondition'].value_counts().to_frame()`\n",
    "- Data sampling: sometimes, it is required to perform over/undersampling in case we have Imbalanced datasets\n",
    "- Data Grouping using groupby : df_group=df[['YearRemodAdd','SalePrice']].groupby(by=['YearRemodAdd']).max()\n",
    "- Data filtering :\n",
    "    - `df_filter =df[df.column>200000]` \n",
    "    - `df_filter =df[(df.column1>150000) & (df.column2==2008)]`\n",
    "    - `df_filter =df[(df.column1>2011) | (df.column2==2008)]`\n",
    "- Data analysis: \n",
    "    - Univariate Analysis : `distplot` and `histplot`\n",
    "    - Bivariate Analysis `pairplot`, `FacetGrid`, `jointplot` etc.\n",
    "    - Multivariate Analysis: correlation matrix or heatmap\n",
    "\n",
    "Notes:\n",
    "- Multivariate analysis involves analyzing the relationship between three or more variables. We can use scatter matrix plots to visualize the relationship between each pair of features, along with the distribution of each feature.\n",
    "- Bivariate analysis involves analyzing the relationship between two variables. We can use scatter plots to visualize the relationship between each pair of feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e5f8f",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699c792",
   "metadata": {},
   "source": [
    "### 1- What does feature engineering mean? \n",
    "\n",
    "Feature engineering refers to the process of raw data manipulation such as addition, deletion, combination, mutation etc. It encompasses the process of creating new features or modifying existing ones to improve the performance of a machine learning model. \n",
    "\n",
    "Here is a range of significant activities used in Feature Engineering :\n",
    "\n",
    "- Feature Selection\n",
    "- Data Transformation\n",
    "- Text Data Processing\n",
    "- Time-Series Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e238ca8",
   "metadata": {},
   "source": [
    "### 2- What does data transformation mean?\n",
    "\n",
    "Data transformation is indeed one subtask within the broader field of feature engineering in machine learning. It is a specific aspect of feature engineering that involves modifying the raw data to make it more suitable for the learning algorithm.\n",
    "It includes : \n",
    "- Feature Scaling\n",
    "- Feature encoding\n",
    "- Feature extraction\n",
    "- Binning or Discretization\n",
    "- Creating Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0d63c",
   "metadata": {},
   "source": [
    "### 3- What does feature scaling mean ?\n",
    "Feature scaling is a preprocessing step in machine learning that involves transforming the numerical features of a dataset to a common scale. Feature scaling is particularly important for algorithms that rely on distance metrics or gradient descent optimization.\n",
    "\n",
    "Here are common techniques for feature scaling:\n",
    "- Normalization\n",
    "- Standard scaling : converts features to standard normal variables (by subtracting the mean and dividing the standard error)\n",
    "- Log scaling or Log transformation\n",
    "- Polynomial transformation\n",
    "- Robust scaling\n",
    "\n",
    "#### 3. 1- Why do we need perform feature scaling ? \n",
    "The goal is to ensure that all features contribute equally to the learning process and to prevent certain features from dominating due to differences in their magnitudes.\n",
    "\n",
    "#### 3. 2- Normalization - Min-Max Scaling\n",
    "- Scales the feature values to a specific range, usually between 0 and 1\n",
    "- Formula : $X_{normalized}= {X-X_{min}\\over X_{max}-X_{min}}$\n",
    "\n",
    "#### 3. 3- Standard scaling - Z-score normalization\n",
    "- Centers the feature values around zero with a standard deviation of 1.\n",
    "- Suitable for algorithms that assume a normal distribution of features.\n",
    "- Formula: $X_{standardized} ={ X - mean(X) \\over std(X)}$\n",
    "\n",
    "#### 3. 4- Robust Scaling\n",
    "- Scales the features based on the interquartile range (IQR) to handle outliers.\n",
    "- Formula: $X_{robust} = {X - median(X)\\over IQR(X)}$\n",
    "\n",
    "#### a. IQR : interquartile range\n",
    "- The IQR is the difference between the third quartile (Q3) and the first quartile (Q1): IQR = Q3 - Q1\n",
    "- Q1: It represents the median of the lower 50% of the data.\n",
    "- Q3 : It represents the median of the upper 50% of the data\n",
    "\n",
    "![title](images/boxplot.png)\n",
    "\n",
    "\n",
    "Here's how you calculate the IQR: \n",
    "- 1. Order the dataset: arrange the values in the dataset in ascending order\n",
    "- 2. Determine the median (Q2): which is the middle value of the dataset. If the dataset has an odd number of observations, the median is the middle value. If it has an even number, the median is the average of the two middle values.\n",
    "- 3. Find the First Quartile (Q1)\n",
    "- 4. Find the Third Quartile (Q3)\n",
    "- 5. Calculate the IQR\n",
    "\n",
    "The IQR provides a robust measure of the spread of the middle 50% of the data, making it less sensitive to extreme values or outliers. It is commonly used in box plots to visually represent the dispersion of data.\n",
    "\n",
    "#### 3. 5- Log Transformation\n",
    "\n",
    "- The log transformation is the most popular among the different types of transformations used in machine learning.\n",
    "- It aims to make highly skewed distributions (features with high variance) less skewed.\n",
    "- The logarithm used is often the natural logarithm (base e) or the common logarithm (base 10).\n",
    "- Generally, we use the natural logarithm function in Log transformation.\n",
    "- If the original data follows a log-normal distribution or approximately so, then the log-transformed data follows a normal or near normal distribution.\n",
    "- However, our real raw data do not always follow a normal distribution. They are often so skewed making the results of our statistical analyses invalid. That’s where Log Transformation comes in.\n",
    "\n",
    "#### 3. 6- Polynomial transformation\n",
    "- It is a feature engineering technique used in machine learning and statistics to capture non-linear relationships between variables.\n",
    "- It involves transforming input features by raising them to the power of an integer, creating polynomial terms. The most common form is the quadratic transformation (squared terms), but higher-order polynomials can also be used.\n",
    "- Such transformations are often beneficial for machine learning algorithms, particularly in tasks involving numerical input variables, improving predictive accuracy, especially in regression tasks.\n",
    "- If X is one input feature ==> $X^2$ is its polynomial feature.\n",
    "- The “degree” of the polynomial is used to control the number of features added, e.g. a degree of 3 will add two new variables for each input variable. Typically a small degree is used such as 2 or 3. Choosing the best polynomial degree is so important as it impacts the number of input features created. \n",
    "\n",
    "**More notes :** \n",
    "\n",
    "- Higher-degree polynomials (Degree > 2) can lead to overfitting, capturing noise in the data rather than true underlying patterns. Regularization techniques may be needed to mitigate this.\n",
    "- It's important to scale features before applying polynomial transformations to prevent features with larger scales from dominating the transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba2d42",
   "metadata": {},
   "source": [
    "### 4- How to deal with categorical values ?\n",
    "- Drop categorical variables\n",
    "- Perform feature encoding\n",
    "\n",
    "#### 4.1- What does feature encoding means? \n",
    "\n",
    "Feature encoding is the process of converting categorical data or text data into a numerical format that can be easily used for machine learning algorithms. In many machine learning models, the input features are expected to be numerical, and encoding is necessary when dealing with non-numeric data.\n",
    "\n",
    "Here are some common encoding methods: \n",
    "- Ordinal encoding: Assign numerical values based on the inherent order of categories\n",
    "- One-hot encoding : Create binary columns for each category, indicating its presence (1) or absence (0)\n",
    "- Label Encoding : Assign a unique numerical label to each category in a categorical variable\n",
    "- Binary Encoding : Convert each category into its binary representation.\n",
    "- Frequency (Count) Encoding: Replace each category with its frequency or count in the dataset\n",
    "\n",
    "\n",
    "**!! Notes :**\n",
    "- Ordianl encoding is a good choice in case we have ranking in our categorical variables (Low, medium, high), most used with DT and Random Forest.\n",
    "- One-hot encoding is more used when there is no ranking in the categorical variables.\n",
    "- If our dataset is very large (high cardinality) --> one-hot encoding can greatly expand the size of dataset : number columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3cbd9",
   "metadata": {},
   "source": [
    "### 5- What does Feature extraction means?\n",
    "One of the primary goals of feature extraction is to reduce the dimensionality of the dataset. High-dimensional data can lead to the curse of dimensionality, making it challenging for models to generalize well.\n",
    "\n",
    "Feature extraction aims to retain the most relevant information from the original data. This involves identifying features that contribute significantly to the variability and patterns within the dataset while discarding redundant or irrelevant information.\n",
    "\n",
    "Here are all types of Feature Extraction:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Singular Value Decomposition (SVD)\n",
    "- Independent Component Analysis (ICA)\n",
    "- Bag-of-Words (BoW)\n",
    "\n",
    "#### 5. 1- What does Principal Component Analysis (PCA) means ? \n",
    "- It is an unsupervised dimensionality reduction technique that aims to transforms input data into a new set of uncorrelated features while keeping the maximum variance in the data.\n",
    "- It can be applied to both supervised and unsupervised machine learning tasks\n",
    "- To calculate it, we can use various python libraries such as `NumPy`, `SciPy`, and `scikit-learn`\n",
    "- It has two main use cases :\n",
    "    - Data Visualization: it aids in visualizing complex datasets, providing valuable insights into the underlying patterns.\n",
    "    - Algorithm Optimization: it can significantly accelerate the learning process of algorithms that may otherwise exhibit slow training speeds.\n",
    "    \n",
    "Here are the steps of calculating PCA using the covariance matrix and use eigenvalue decomposition to obtain the eigenvectors and eigenvalues. Here are the steps to apply :\n",
    " - 1. Standardise the data\n",
    " - 2. Compute the covariance matrix and use eigenvalue decomposition to obtain the eigenvectors and eigenvalues.\n",
    " - 3. Select the k largest eigenvalues and their associated eigenvectors.\n",
    " - 4. Transform the data into a k dimensional subspace using those k eigenvectors.\n",
    "    \n",
    "#### a. How to choose the correct number of PCA Components ?\n",
    "\n",
    "#### b. Why do we need to find eigenvalues and eigenvectors?\n",
    "Because the principal component directions are given by the eigenvectors of the matrix, and the magnitudes of the components are given by the eigenvalues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5cb2a",
   "metadata": {},
   "source": [
    "#### 5. 2- What does Singular Value Decomposition (SVD) means ? \n",
    "Singular Value Decomposition (SVD) is a mathematical technique widely used in linear algebra and numerical analysis. \n",
    "\n",
    "It is a method for decomposing a matrix into three other matrices, which can be helpful in various applications, including signal processing, data analysis, and machine learning. The SVD of a matrix A is represented as:\n",
    "\n",
    "$ A = U Σ V^T $ \n",
    " \n",
    "\n",
    "Here's a breakdown of the terms:\n",
    "\n",
    "- A: The original matrix that we want to decompose.\n",
    "- U: The left singular vectors matrix. Columns of U are the eigenvectors of $AA^T$ (covariance matrix of A).\n",
    "- Σ: The diagonal matrix of singular values. The singular values are the square roots of the eigenvalues of $AA^T$ OR $A^TA$. They represent the magnitude of the singular vectors.\n",
    "\n",
    "- $V^TV$: The transpose of the right singular vectors matrix. Columns of V are the eigenvectors of $A^TA$ (or $AA^T$) \n",
    " \n",
    "SVD has several applications and implications:\n",
    "\n",
    "- **Dimensionality Reduction** \n",
    "- **Image Compression**\n",
    "- **Pseudo-Inverse** \n",
    "- **Collaborative Filtering**\n",
    "- **Latent Semantic Analysis (LSA)** \n",
    "\n",
    "#### 5. 3 - What are the different dimensionality reduction techniques?\n",
    "\n",
    "#### 5. 3- What does Independent Component Analysis means ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621b3f93",
   "metadata": {},
   "source": [
    "### 6- How to perform Text Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d92f9",
   "metadata": {},
   "source": [
    "### 7- How to perform Time-Series Feature Engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c90b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c5a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
