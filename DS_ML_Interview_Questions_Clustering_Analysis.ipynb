{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379570e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0913a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1c9ac0f",
   "metadata": {},
   "source": [
    "### 5- Examples of well-known machine learning algorithms used to solve clustering problems\n",
    "\n",
    "Several well-known machine learning algorithms are commonly used for solving clustering problems. Here are some examples:\n",
    "\n",
    "- K-Means Clustering \n",
    "- Hierarchical Clustering\n",
    "- DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "- Mean Shift\n",
    "- Gaussian Mixture Model (GMM)\n",
    "- Agglomerative Clustering\n",
    "\n",
    "These algorithms address different types of clustering scenarios and have varying strengths depending on the nature of the data and the desired outcomes. The choice of clustering algorithm often depends on factors such as the shape of clusters, noise in the data, and the number of clusters expected.\n",
    "\n",
    "#### 5.1- K-Means \n",
    "- Most known and used clsutering algorithm\n",
    "- Has two version : Hard-Kmeans and Soft Kmeans\n",
    "- Steps of Hard- Kmeans:\n",
    "    - Choose number of clusters K\n",
    "    - start with initial guess : xk(0), k=1.....K\n",
    "    - etc.\n",
    "- Formula : y(n)=x(n)+v(n) :\n",
    "    - y(n): data point\n",
    "    - x(n): centroid \n",
    "    - v(n): Gausian noise (Gaussian, statistically idependent , mean=0 and ${\\sigma_{k}^2}$=variance\n",
    "- This algorithm has two disadvantages (problem with linear complexity):\n",
    "    - Number of clusters \n",
    "    - Random choice for the centroid : different choices can led to different results\n",
    "#### a- What soft Kmeans mean?\n",
    "- Does not require hard decision in which y(n) belongs to one and only one decision region\n",
    "- y(n) has different probabilityy that belongs to a cluster K\n",
    "- Centroid xk is calculated in function of weights assigned to each point\n",
    "#### b- What is the K-meadian\n",
    "- It is a clustering algorithm that uses the median to calculate the updated center/ centroid of group\n",
    "- why??? ==> median is less affected by outliers but this method is much slower for large datasets because sorting is required on each iteration to compute median\n",
    "\n",
    "#### 5.2- What is Hierarchical Clustering ? \n",
    "- It is a clustering technique used in data analysis and machine learning.\n",
    "- It starts with each data point as a separate cluster and then iteratively merges or splits clusters based on their similarity, forming a dendrogram.\n",
    "- Dendrogram is representation of clusters hierarchy, with the vertical lines indicating the merging or splitting points.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/Dendrogram.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "- Here are the key characteristics and steps used in hierarchical clustering:\n",
    "\n",
    "    1. Each data point starts as a singleton cluster\n",
    "    2. The similarity or dissimilarity between data points is calculated using a chosen distance metric. \n",
    "    3. Based on the similarity/dissimilarity values, the algorithm either merges clusters or splits data points into new clusters.\n",
    "    4. After merging or splitting clusters, a dendrogram is constructed.\n",
    "    5. The algorithm continues until all data points belong to a single cluster or until a predefined stopping criterion is met.\n",
    "\n",
    "- Hierarchical clustering can be classified into two main types:\n",
    "    - Agglomerative (Bottom-Up) Hierarchical Clustering: it starts with individual data points as separate clusters and merges them iteratively to form larger clusters.\n",
    "    - Divisive (Top-Down) Hierarchical Clustering: it starts with all data points in a single cluster and recursively splits them into smaller clusters.\n",
    "\n",
    "#### a. Advantages : \n",
    "- Does not require number of clusters in advance.\n",
    "- Easy to implement.\n",
    "- Produces a dendrogram which helps with understanding the data.\n",
    "- Intuitive visualization with dendrograms\n",
    "- It is possible to cut dendrogram if we want to change the number of clusters. \n",
    "- Captures the hierarchical structure of the data\n",
    "#### b. Disadvantages:\n",
    "- Computationally more intensive, especially for large datasets. \n",
    "- Need to identify distance between two observations or between two clusters\n",
    "- sometimes it is difficult to identify number of clusters. \n",
    "- Lack of flexibility when dealing with non-globular shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a74971",
   "metadata": {},
   "source": [
    "#### 5.3. What distance metrics are used as similarity measures between two samples in clustering? \n",
    "- Several distance metrics are commonly used as similarity measures between two samples in data analysis and clustering.\n",
    "- The choice of distance metric depends on the nature of the data and the characteristics of the analysis.\n",
    "- Here are some commonly used distance metrics:\n",
    "    - Eculidian Distance \n",
    "    - Manhattan Distance \n",
    "    - Maximum Distance  \n",
    "    - Minkowski Distance \n",
    "    - Chebyshev Distance\n",
    "    - Hamming Distance\n",
    "    - Cosine Similarity\n",
    "    - Correlation Distance\n",
    "    - Jaccard Similarity Coefficient\n",
    "    \n",
    "Here are more details regarding each distance metric : \n",
    " - **Eculidian Distance:**\n",
    "     - Measures the straight-line distance between two points in Euclidean space.\n",
    "     - Suitable for continuous numerical data\n",
    "     - Formula: \n",
    "- **Manhattan Distance:**\n",
    "    - Calculates the sum of the absolute differences between the coordinates of two points.\n",
    "    - Suitable for data with attributes that have different units or scales.\n",
    "    - Formula: \n",
    "- **Minkowski Distance:** \n",
    "    - Generalizes both Euclidean and Manhattan distances.\n",
    "    - Parameterized by a parameter 'p,' where p = 1 corresponds to Manhattan distance, and p = 2 corresponds to Euclidean distance.\n",
    "    - Formula : \n",
    "- **Chebyshev Distance:**\n",
    "    - Measures the maximum absolute difference between coordinates.\n",
    "    - Suitable for data where outliers might have a significant impact.\n",
    "    - Formula: \n",
    "- **Hamming Distance:**\n",
    "    - Measures the number of positions at which corresponding symbols differ in two binary strings.\n",
    "    - Suitable for categorical data or binary data.\n",
    "    - Formula : \n",
    "- **Cosine Similarity:**\n",
    "    - Measures the cosine of the angle between two vectors.\n",
    "    - Suitable for text data, document clustering, and cases where the magnitude of the data points is less important.\n",
    "- **Correlation Distance:**\n",
    "    - Measures the similarity in shape between two vectors, taking into account the correlation between variables.\n",
    "    - Suitable for datasets where the relative changes in variables are more important than their absolute values.\n",
    "- **Jaccard Similarity Coefficient:**\n",
    "    - Measures the similarity between two sets by comparing the size of their intersection to the size of their union.\n",
    "    - Suitable for binary or categorical data.\n",
    "- **Mahalanobis Distance:**\n",
    "    - Takes into account the correlation between variables and the scales of the variables.\n",
    "    - Suitable for datasets with correlated variables and different variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44396a23",
   "metadata": {},
   "source": [
    "#### d. How to calculate distance between two clusters ?\n",
    "- The distance between two clusters, often referred to as linkage or proximity.\n",
    "- These distance measures are used during the agglomeration process in hierarchical clustering.\n",
    "- The algorithm iteratively merges clusters based on the chosen linkage method until all data points belong to a single cluster.\n",
    "- The choice of linkage method can impact the resulting dendrogram and the interpretation of the cluster structure.\n",
    "- Different linkage methods may be suitable for different types of data and clustering objectives.\n",
    "- Here are some commonly used linkage methods:\n",
    "    - Single Linkage (Minimum Linkage)\n",
    "    - Complete Linkage (Maximum Linkage)\n",
    "    - Average Linkage\n",
    "    - Centroid Linkage (UPGMA - Unweighted Pair Group Method with Arithmetic Mean)\n",
    "    - Ward's Method\n",
    "\n",
    "Here are more details regarding each method:\n",
    "- **Single Linkage (Minimum Linkage):**\n",
    "    - The distance between two clusters is the minimum distance between any two points in the two clusters.\n",
    "    - Formula: $d(C_{1},C_{2})={min_{i \\in C_{1}, j \\in C_{2}} distance(i,j)}$\n",
    "   \n",
    "- **Complete Linkage (Maximum Linkage):**\n",
    "    - The distance between two clusters is the maximum distance between any two points in the two clusters.\n",
    "    - Formula: $d(C_{1},C_{2})={max{i \\in C_{1}, j \\in C_{2}} distance(i,j)}$ \n",
    "- **Average Linkage:**\n",
    "    - The distance between two clusters is the average distance between all pairs of points from the two clusters.\n",
    "    - Formula: $d(C_{1},C_{2})={1 \\over |C_{1}|\\times|C_{2}|}{\\sum \\limits _{i\\in C_{1}} \\sum \\limits _{j\\in C_{2}} distance(i,j)}$\n",
    "- **Centroid Linkage (UPGMA - Unweighted Pair Group Method with Arithmetic Mean):**\n",
    "    - The distance between two clusters is the distance between their centroids (mean vectors).\n",
    "    - Formula: d($C_{1}$,$C_{2}$)=distance(centroid($C_{1}$), centroid($C_{2}$))\n",
    "- **Ward's Method:**\n",
    "    - Minimizes the increase in variance within the clusters when merging.\n",
    "    - It considers the sum of squared differences within each cluster and the sum of squared differences between the centroids of the clusters.\n",
    "    - Formula: $d(C_{1},C_{2})= {\\sqrt{{|C_{1}|\\times|C_{2}|}\\over {|C_{1}|+|C_{2}|}}}distance(centroid(C_{1}), centroid(C_{2}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f87420",
   "metadata": {},
   "source": [
    "### 17- What are the performance metrics for Clustering ?\n",
    "- Evaluating the performance of clustering algorithms is less straightforward compared to supervised learning tasks like classification.\n",
    "- Clustering is often exploratory, and there may not be explicit labels for assessing correctness.\n",
    "- Several metrics and methods are commonly used to assess the quality of clustering results. Here are some performance metrics for clustering:\n",
    "    - Silhouette Score\n",
    "    - Davies-Bouldin Index\n",
    "    - Calinski-Harabasz Index (Variance Ratio Criterion)\n",
    "    - Inertia (Within-Cluster Sum of Squares)\n",
    "    - Normalized Mutual Information (NMI)\n",
    "    - Cluster Purity\n",
    "#### 17.1 How to compare two different clustering ?\n",
    "- We can use the SSE: SUM of Squared Error \n",
    "- Formula: $SSE={\\sum \\limits _{k=1} ^{K} \\sum \\limits _{y_{i} \\in C_{k}} ||y_{i}-x_{k}||^2}$\n",
    "- $y_{i}$ : is the ith vector belonging to cluster $C_{k}$ and $x_{k}$ is the centroid \n",
    "- Formula of centroid: $$x_{k}={1\\over N_{k}}{\\sum \\limits _{y_{i} \\in C_{k}} y_{i}}$$\n",
    "- If SSE is small, clusters are compact and well separated\n",
    "- Cluster with smallest SSE is the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede6dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5f36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370bb8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6b60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
