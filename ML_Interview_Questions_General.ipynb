{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20126c01",
   "metadata": {},
   "source": [
    "# Data Scientist Interview Questions\n",
    "## Part 3 : ML : modelling and evaluation\n",
    "\n",
    "This Jupyter notebook serves as a focused resource for individuals gearing up for technical interviews in the fields of machine learning engineering and data science. It specifically delves into questions related to all phases of machine learning model evaluation and deployment. Whether you're a candidate looking to sharpen your interview skills or an interviewer seeking insightful questions, this notebook provides valuable content for honing your understanding of machine learning evaluation and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27572c9",
   "metadata": {},
   "source": [
    "## 0- What does Machine Learning mean ? \n",
    "Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to perform tasks without explicit programming. The core idea behind machine learning is to allow machines to learn patterns, make predictions, or optimize decisions based on data.\n",
    "\n",
    "Key concepts in machine learning include:\n",
    "- Types of Machine Learning: supervised, unsupervised and semi-supervised\n",
    "- Types of machine learning problems: classification, regression and clustering\n",
    "- Split data into Training, validation and testing sets (case of supervised)\n",
    "- Choose the right algorithm depends on the problem you want to solve\n",
    "- Model Evaluation\n",
    "- Hyperparameter Tuning\n",
    "- Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e52dfd",
   "metadata": {},
   "source": [
    "## 1- What are three stages of building a machine learning model ? \n",
    "- The process of building a machine learning model includes three main stages, These stages are:\n",
    "    - **Training phase:** after splitting the data into training and testing sets, training data is used to train our model on a labeled dataset. During the training phase, the model tries to learn relationships between input data and the corresponding output target values while adjusting its internal parameters. Throughout this phase, the model aims to maximise the accuracy of making precise predictions or classifications when exposed to unseen data.\n",
    "    - **Validation phase:** after the model is well trained, we evaluate it on a seperate dataset known as the validation set (maximum 10% of our data). This dataset is not used during the training process. Validation stage helps identify the existence of certain overfitting (model performing well on training data but poorly on new data) or certain underfitting (model needs more training to capture the underlying patterns in the data).\n",
    "    - **Testing (Inference) phase:** during this phase, the trained and validated model is applied to unseen dataset, called test dataset. This phase aims to evaluate the model's performance and provides a measure regarding the model's effectiveness and its ability to make accurate predictions in a production environment.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb4290",
   "metadata": {},
   "source": [
    "## 2- How to split your data while building a machine learning model ?    \n",
    "- During the model building phase, it is required to split the data into three main sets to evaluate the model's performance and effectiveness. The three sets are: \n",
    "    - Training: used to train the model and learn relationship between inputs and outputs, contains 70-80% of our total dataset\n",
    "    - Validation: used to validate the model, fine-tune the model's hyperparameters and assess its performance during training, it helps to prevent overfitting and underfitting. It contains 10-15% of the total data\n",
    "    - Testing: used to test and evaluate the model's performance against unseen data and after validation phase. It is used to measure how effective will our built model be in a production environment. It contains 10-15% of the total data.\n",
    "\n",
    "- Splitting data is accomplished after the preprocessing phase (handle missing values, categorical features, scale features, etc.). \n",
    "- It is important to ensure that the split is representative of the overall distribution of the data to avoid biased results.\n",
    "- It is favorable to use cross-validation technique. \n",
    "- No fixed rule to split data between training, validation and testing, portions can vary based on individual preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db499b",
   "metadata": {},
   "source": [
    "## 3- What are the types of ML algorithms ? \n",
    "Machine learning algorithms can be categorized into several types based on their learning styles and the nature of the task they are designed to solve.\n",
    "\n",
    "Here are some common types of machine learning algorithms:\n",
    "- **Supervised Learning** \n",
    "- **Unsupervised Learning**\n",
    "- **Semi-Supervised Learning**\n",
    "- **Deep Learning** \n",
    "- **Reinforcement Learning** \n",
    "- **Ensemble learning**  \n",
    "- **Ranking**\n",
    "- **Recommendation system** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47970ec3",
   "metadata": {},
   "source": [
    "## 4- What does supervised, unsupervised and semi-supervised mean in ML? \n",
    "\n",
    "In machine learning, the terms \"supervised learning,\" \"unsupervised learning,\" and \"semi-supervised learning\" refer to different approaches based on the type of training data available and the learning task at hand:\n",
    "\n",
    "- **Supervised Learning :** training a model on a labeled dataset, where the algorithm learns the relationship between input features and corresponding target labels. Can be used for Regression (continous output) or Classification (discrete output). \n",
    "- **Unsupervised Learning :** Deals with unlabeled data and aims to find patterns, structures, or relationships within the data. Can be used for Clustering (Groups similar data points together) or association\n",
    "- **Semi-Supervised Learning:** Utilizes a combination of labeled and unlabeled data to improve learning performance, often in situations where obtaining labeled data is challenging and expensive.\n",
    "\n",
    "### 4.1- What are Unsupervised Learning techniques ?\n",
    " We have two techniques, Clustering and association: \n",
    " - Custering :  involves grouping similar data points together based on inherent patterns or similarities. Example: grouping customers with similar purchasing behavior for targeted marketing.. \n",
    " - Association : identifying patterns of associations between different variables or items. Example: e-commerse website suggest other items for you to buy based on prior purchases.\n",
    " \n",
    "### 4.2- What are Supervised Learning techniques ? \n",
    "We have two techniques: classfication and regression: \n",
    "- Regression : involves predicting a continuous output or numerical value based on input features. Examples : predicting house prices, temperature, stock prices etc.\n",
    "- Classification : is the task of assigning predefined labels or categories to input data. We have two types of classification algorithms: \n",
    "    - Binary classification (two classes). Example: identifying whether an email is spam or not.\n",
    "    - Multiclass classification (multiple classes). Example: classifying images of animals into different species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb48e0",
   "metadata": {},
   "source": [
    "## 5- Examples of well-known machine learning algorithms used to solve Regression problems\n",
    "\n",
    "Here are some well-known machine learning algorithms commonly used to solve regression problems:\n",
    "\n",
    "- Linear Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Gradient Boosting Algorithms (e.g., XGBoost, LightGBM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Bayesian Regression\n",
    "- Lasso Regression\n",
    "- Ridge Regression\n",
    "- Neural Networks (Deep Learning)\n",
    "\n",
    "More details regarding each algorithm could be found in `DS_ML_Interview_Questions_Regression_Analysis` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b2972",
   "metadata": {},
   "source": [
    "## 6- Examples of well-known machine learning algorithms used to solve Classification problems\n",
    "\n",
    "Here are some well-known machine learning algorithms commonly used to solve classification problems:\n",
    "\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Support Vector Machines (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Naive Bayes\n",
    "- AdaBoost\n",
    "- Gradient Boosting Machines (GBM)\n",
    "- XGBoost\n",
    "- CatBoost\n",
    "- LightGBM\n",
    "- Neural Networks (Deep Learning)\n",
    "\n",
    "More details regarding each algorithm could be found in `DS_ML_Interview_Questions_Classification_Analysis`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064a5d0",
   "metadata": {},
   "source": [
    "## 7- Examples of well-known machine learning algorithms used to solve Clustering problems\n",
    "\n",
    "Several well-known machine learning algorithms are commonly used for solving clustering problems. Here are some examples:\n",
    "\n",
    "- K-Means Clustering \n",
    "- Hierarchical Clustering\n",
    "- Agglomerative Clustering\n",
    "- DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "- Mean Shift\n",
    "- Gaussian Mixture Model (GMM)\n",
    "\n",
    "These algorithms address different types of clustering scenarios and have varying strengths depending on the nature of the data and the desired outcomes. The choice of clustering algorithm often depends on factors such as the shape of clusters, noise in the data, and the number of clusters expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943cf40",
   "metadata": {},
   "source": [
    "## 8- How to choose which ML algorithm to use given a dataset?\n",
    "- Choosing the right machine learning algorithm for a given dataset involves considering various factors related to the nature of the data and the problem at hand.\n",
    "- No master algorithm it all depends on the situation\n",
    "- Here's a step-by-step guide to take the right decision : \n",
    "    - **Understand the Problem or the situation :**\n",
    "        - Understanding the nature of taregt variable (continous, categorical?, is all output variables labled or mixed?). \n",
    "        - Determine the problem we are trying to solve ( is it classification, regression or clustering?)\n",
    "    - **Domain Knowledge:**\n",
    "        - Trying to find any domain-specific knowledge that might influence the choice of algorithm. \n",
    "        - Certain algorithms may be well-suited to specific industries or types of problems.\n",
    "    - **Explore the Data:**\n",
    "        - Determine data dimension \n",
    "        - Perform exploratory data analysis (EDA) to understand the characteristics of the dataset.\n",
    "        - Understand features distribution, identify patterns and detect outliers etc.\n",
    "    - **Consider the Size of the Dataset:**\n",
    "        - Small: simpler models or models with fewer parameters may be more suitable to avoid overfitting.\n",
    "        - Large: more complex models can be considered.\n",
    "    - **Check for Linearity:** \n",
    "        - Studying the relationships between features and the target variable are linear or nonlinear.\n",
    "        - If linear: then use linear models as they are more effective in this case.\n",
    "        - If nonlinear: then non linear models (e.g., decision trees, support vector machines) may be suitable for more complex relationships.\n",
    "    - **Data pre-processing :**\n",
    "        - Handle Categorical Data : some algorithms handle categorical data naturally (e.g., decision trees), while others may require encoding.\n",
    "        - Dealing with Missing Values: some algorithms can handle missing data, while others may require imputation or removal of missing values.\n",
    "        - Check for Outliers: some algorithms may be sensitive to outliers, while others are more robust.\n",
    "    - **Consider the Speed and Scalability:**\n",
    "        - Take into account the computational requirements of the algorithm.\n",
    "        - Some algorithms are faster and more scalable than others, making them suitable for large datasets.\n",
    "    - **Evaluate Model Complexity:**\n",
    "        - Simple models like linear regression are interpretable but may not capture complex patterns. \n",
    "        - More complex models like ensemble methods (e.g., random forests, gradient boosting) can capture intricate relationships but may be prone to overfitting.\n",
    "    - **Validation and Cross-Validation:**\n",
    "        - Use validation techniques, such as cross-validation, to assess the performance of different algorithms.\n",
    "        - This helps you choose the one that generalizes well to new, unseen data.\n",
    "    - **Experiment and Iterate:**\n",
    "        - It's often beneficial to experiment with multiple algorithms and compare their performance.\n",
    "        - Iterate on the choice of algorithm based on performance metrics and insights gained during the modeling process.\n",
    "\n",
    "**Note:**\n",
    "- There is no one-size-fits-all solution, and the choice of the algorithm may involve some trial and error.\n",
    "- Additionally, ensemble methods, which combine multiple models, can sometimes provide robust solutions.\n",
    "- Keep in mind that the performance of an algorithm depends on the specific characteristics of your dataset and the goals of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce66ed5",
   "metadata": {},
   "source": [
    "## 9- What is Ensemble learning?\n",
    "\n",
    "Ensemble learning is a machine learning technique that involves combining the predictions of multiple individual models to improve overall performance and accuracy. Instead of relying on a single model, ensemble methods leverage the strengths of diverse models to compensate for each other's weaknesses. The idea is that by aggregating the predictions of multiple models, the ensemble can achieve better generalization and make more robust predictions than any individual model.\n",
    "\n",
    "There are several ensemble learning methods, with two primary types being:\n",
    "- **Bagging (Bootstrap Aggregating) :** \n",
    "    - Involves training multiple instances of the same model on different subsets of the training data, typically sampled with replacement. \n",
    "    - Examples : Random Forest, Bagged Decision Trees, Bagged SVM (Support Vector Machines), Bagged K-Nearest Neighbors, Bagged Neural Networks\n",
    "- **Boosting :**\n",
    "    - Focuses on sequentially training models, with each subsequent model giving more attention to the instances that the previous models misclassified. \n",
    "    - Examples: AdaBoost (Adaptive Boosting), Gradient Boosting, XGBoost (Extreme Gradient Boosting), LightGBM (Light Gradient Boosting Machine), CatBoost, GBM (Gradient Boosting Machine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b831fc9",
   "metadata": {},
   "source": [
    "## 10- What is Reinforcement Learning?\n",
    "\n",
    "Is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, allowing it to learn optimal strategies over time to maximize cumulative rewards. It is inspired by the way humans and animals learn from trial and error.\n",
    "\n",
    "Here are some applications of Reinforcement Learning : \n",
    "- Automated Robots\n",
    "- Natural Language Processing\n",
    "- Marketing and Advertising \n",
    "- Image Processing\n",
    "- Recommendation Systems\n",
    "- Traffic Control \n",
    "- Healthcare \n",
    "- Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176b31a",
   "metadata": {},
   "source": [
    "## 11- What is Recommender Systems\n",
    "Also known as recommendation systems or engines, are applications or algorithms designed to suggest items or content to users based on their preferences and behavior. These systems leverage data about users and items to make personalized recommendations, aiming to enhance user experience and satisfaction. There are two main types of recommender systems:\n",
    "\n",
    "- Content-Based Recommender Systems\n",
    "- Collaborative Filtering Recommender Systems\n",
    "Recommender systems are widely used in various industries, including e-commerce, streaming services, social media, and more. They help users discover new items, increase user engagement, and contribute to business success by promoting relevant content and products\n",
    "\n",
    "### 11. 1- What is Content-Based Recommender Systems ? \n",
    "### 11. 2- What is Collaborative Filtering Recommender Systems ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba017a",
   "metadata": {},
   "source": [
    "## 12- What is Ranking ? \n",
    "\n",
    "Ranking in machine learning refers to the process of assigning a meaningful order or ranking to a set of items based on their relevance or importance. This is often used in scenarios where the goal is to prioritize or sort items based on their predicted or observed characteristics.\n",
    "\n",
    "Ranking problems are common in various applications, including information retrieval, recommendation systems, and search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d9b66",
   "metadata": {},
   "source": [
    "## 13- What is Overfitting, causes and mitigation? \n",
    "\n",
    "- Overfitting is a common challenges in machine learning that relate to the performance of a model on unseen data.\n",
    "- It occurs when a machine learning model learns the training data too well, capturing noise and random fluctuations in addition to the underlying patterns (as concept).\n",
    "- High error on testing dataset.\n",
    "\n",
    "### 13. 1-Key characteristics of Overfitting :\n",
    "\n",
    "- Excellent Performance on Training Data.\n",
    "- Poor Generalization to New Data\n",
    "- Low Bias, High Variance: the model is not biased toward any particular assumption, but its predictions are highly sensitive to variations in the training data.\n",
    "- Complex Model: overfit models are often overly complex and may capture noise or outliers as if they were significant patterns.\n",
    "- Memorization of Training Data: instead of learning the underlying patterns, an overfit model may memorize specific details of the training data, including noise and outliers.\n",
    "\n",
    "### 13. 2- Causes of Overfitting : \n",
    "- Too many features or parameters.\n",
    "- Model is too complex for the available data.\n",
    "- Training on a small dataset or training for too many iterations\n",
    "\n",
    "### 13. 3- Overfitting mitigation :\n",
    "- Regularization techniques (e.g., L1 or L2 regularization).\n",
    "- Feature selection or dimensionality reduction.\n",
    "- Increasing the amount of training data.\n",
    "- Using simpler model architectures with less variables and parameters so variance can be reduced.\n",
    "- Use of Cross-validation method\n",
    "\n",
    "**Note:**\n",
    "- It is important to find balance between model complexity and the ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4797651",
   "metadata": {},
   "source": [
    "## 14- What is Underfitting, causes and mitigation? \n",
    "- It is the case when the model is too simple to capture the underlying patterns in the training data.\n",
    "- Besides, the model performs poorly not only on the training data but also on new, unseen data.\n",
    "- High error rate on both training and testing datasets.\n",
    "- It occurs when the model lacks the complexity or flexibility to represent the underlying relationships between the features and the target variable.\n",
    "\n",
    "### 14. 1-Key characteristics of underfitting :\n",
    "- Poor Performance on Training Data\n",
    "- Poor Generalization to New Data\n",
    "- High Bias, Low Variance : The model is biased toward making overly simple assumptions about the data.\n",
    "- Inability to Capture Patterns\n",
    "- Simplistic Model: underfit models are often too simplistic and may not capture the nuances or complexities present in the data.\n",
    "\n",
    "### 14. 2- Causes of Underfitting: \n",
    "- Too few features or parameters: inadequate feature representation.\n",
    "- Insufficient model complexity: using a model that is too basic for the complexity of the data.\n",
    "- Inadequate training time or data.\n",
    "\n",
    "\n",
    "### 14. 3- Underfitting mitigation: \n",
    "- Increasing the complexity of the model\n",
    "- Adding relevant features.\n",
    "- Training for a longer duration.\n",
    "- Considering more sophisticated model architectures.\n",
    "\n",
    "**Note:**\n",
    "- Increasing model complexity excessively may lead to overfitting. \n",
    "- Achieving a balance between overfitting and underfitting is crucial.\n",
    "- This balance, often referred to as the model's \"sweet spot\", results in a model that generalizes well to new, unseen data. \n",
    "- Techniques like cross-validation, hyperparameter tuning, and monitoring learning curves can help strike this balance during the model development process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b8142",
   "metadata": {},
   "source": [
    "## 15- What are the types of Regularization in Machine Learning\n",
    "\n",
    "- Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the objective/loss function.\n",
    "- It consists on adding a cost term that penalize the large weights of model.\n",
    "- There are mainly two types of regularization commonly used: L1 regularization (Lasso) and L2 regularization (Ridge). - Additionally, Elastic Net is a combination of both L1 and L2 regularization. \n",
    "\n",
    "Here are all the used techniques in ML : \n",
    "- L1 Regularization (Lasso)\n",
    "- L2 Regularization (Ridge)\n",
    "- Elastic Net\n",
    "\n",
    "### 15. 1 - L1 Regularization (Lasso) : \n",
    "L1 regularization tends to shrink some coefficients exactly to zero, effectively excluding the corresponding features from the model. It is often used when there is a belief that some features are irrelevant. The penalty term is the sum of the absolute values of the regression coefficients.\n",
    "\n",
    "### 15. 2 - L2 Regularization (Ridge) : \n",
    "\n",
    "L2 regularization tends to shrink coefficients toward zero without eliminating them entirely. It is effective in dealing with multicollinearity (high correlation between predictors) and preventing overfitting. The penalty term is the sum of the squared values of the regression coefficients.\n",
    "\n",
    "\n",
    "### 15. 3 - Elastic Net: \n",
    "\n",
    "Elastic Net combines both L1 and L2 penalties in the objective function. It has two control parameters, alpha (which controls the overall strength of regularization) and the mixing parameter, which determines the ratio between L1 and L2 penalties. It is useful when there are many correlated features, and it provides a balance between Lasso and Ridge.\n",
    "\n",
    "**Note:**\n",
    "- These regularization techniques help improve the generalization performance of machine learning models by preventing them from becoming too complex and fitting noise in the training data.\n",
    "- The choice between L1, L2, or Elastic Net depends on the specific characteristics of the dataset and the modeling goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b70ac",
   "metadata": {},
   "source": [
    "## 16- What is Model Validation Technique?\n",
    "\n",
    "Validation techniques in machine learning are essential for assessing the performance of a model and ensuring its ability to generalize well to unseen data. \n",
    "\n",
    "Here are some common validation techniques:\n",
    "- Train-Test Split \n",
    "- K-Fold Cross-Validation \n",
    "- Stratified K-Fold Cross-Validation\n",
    "- Leave-One-Out Cross-Validation (LOOCV)\n",
    "- Holdout Validation\n",
    "- Time Series Cross-Validation\n",
    "\n",
    "### 16. 1 - What is train-test-validation split?\n",
    "- It is an important step to indicate how well a model will perform with real-world, unseen data.\n",
    "- A good train-test-validation split helps mitigate overfitting and ensures that evaluation is not biased.\n",
    "- It consists on dividing input dataset into three subsets:\n",
    "    - Training: 70-80% of the data\n",
    "    - Validation: 10-15% of the data\n",
    "    - Testing: 10-15% of the data\n",
    "- This split aims to ensure that the model is trained on a sufficiently large dataset, validated on a separate set to fine-tune parameters, and tested on a completely independent set to provide an unbiased evaluation of its performance.\n",
    "\n",
    "### 16. 2 - What is K-Fold Cross-Validation?\n",
    "- It is a technique used to assess the performance and generalization ability of a model. \n",
    "- The input dataset will be divided into k equally sized folds/groups.\n",
    "- (K-1) folds are used for training and one fold is used for testing. Then, we evaluate the model. \n",
    "- Repeating the training and evaluation K times.\n",
    "- Each time a different fold is taken as the test set while the remaining data is used for training.\n",
    "- Here are the steps of the process :\n",
    "    - Data Splitting\n",
    "    - Model Training and Evaluation : iteration\n",
    "    - Performance Metrics : error, accuracy, recall, precision etc is evaluated for each iteration.\n",
    "    - Average Performance : average performance (error) is evaluated across all K iterations ==> provide a more reliable estimate of the model's performance.\n",
    "- Error formula : $e(n)={y(n)-\\hat y(n)}$ is calculated for each iteration where $\\hat y$ is the predicted value.\n",
    "- Ideally, K is 5 or 10. The optimal value may depend on the size and nature of the dataset.\n",
    "- A higher K value can result in a more reliable performance estimate but may increase computational costs.\n",
    "- K-fold is very helpful to limit issues related to the variability of a single train-test split.==> It provides a more robust evaluation of a model's performance by ensuring that every data point is used for testing exactly once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598bc8b",
   "metadata": {},
   "source": [
    "### 16. 3 - What is Stratified K-Fold Cross-Validation? \n",
    "- It is an extension of K-Fold Cross-Validation that ensures the distribution of the target variable's classes is approximately the same in each fold as it is in the entire dataset.\n",
    "- In case of imbalanced datasets, this technique is prefered because some classes may be underrepresented.\n",
    "- It helps in addressing issues related to  overrepresented or underrepresented classes in specific folds, which could lead to biased model evaluations.\n",
    "- Here are the main steps for Stratified K-Fold Cross-Validation :\n",
    "    - Data Splitting : ensuring that each fold has an equal distribution for each class samples.\n",
    "    - Model Training and Evaluation: the same K-fold cross-validation, steps repeated K times\n",
    "    - Average Performance : the average performance is calculated at the end of all K iterations to provide a robust performance estimate.\n",
    "    \n",
    "### 16. 4 - What is Leave-One-Out Cross-Validation (LOOCV)?\n",
    "- It is a specific case of k-fold cross-validation where the number of folds (K) is set equal to the number of data points in the dataset. \n",
    "- Each iteration one point is dedicated to testing while the remaining samples are dedicated for training\n",
    "- The same as k-fold, we calculate the performance metric for each iteration then we evaluate the average.\n",
    "- The process is repeated until each data point has been used as a test set exactly once.\n",
    "- It has the next advantages: \n",
    "    - It minimizes bias introduced by the choice of a specific train-test split.\n",
    "    - It provides a robust estimate of a model's performance since each data point serves as both training and test data.\n",
    "    - It is computationally expensive, especially for large datasets, as we are going to create a model for each sample.\n",
    "    - It is preferably to be used with only small datasets.\n",
    "### 16. 5 - What is Holdout Validation ?\n",
    "- It is known as a train-test split. \n",
    "- The input dataset will be divided into two subsets: a training set (70-80%) and a testing set (20-30%).\n",
    "- The exact split ratio depends on factors such as the size of the dataset and the nature of the machine learning task.\n",
    "- The testing set is called Holdout Set also and it helps gathering an initial estimate of a model's performance.\n",
    "- The performance metrics are accuracy, precision, recall, error, etc\n",
    "- This technique is suitable if the input dataset is large enough to provide sufficient data for both training and testing, and when computational resources are limited compared to more computationally intensive methods like cross-validation.\n",
    "- This technique could be not too reliable as the model performance can be influenced by the specific random split of data into training and testing sets. \n",
    "- To address this variability, multiple iterations of the holdout process can be performed, and the results can be averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdc64f",
   "metadata": {},
   "source": [
    "## 17- What does correlation mean in ML?\n",
    "- It corresponds to the statistical relationship between two variables x and y. \n",
    "- It measures the degree to which changes in one variable correspond to changes in another variable.\n",
    "- It does not imply causation, it only quantifies the strength and direction of the linear relationship between variables.\n",
    "- The Pearson correlation coefficient r, is the most used correlation measure and it ranges from -1 to 1: \n",
    "    - If **r > 0: Positive Correlation**, if one variable increases, the other variable tends to increase as well.\n",
    "    - If **r < 0: Negative Correlation**, if one variable increases, the other variable tends to decrease.\n",
    "    - If **r = 0: Zero Correlation**, indicates no linear relationship between the variables.\n",
    "- Here are some key points about correlation: \n",
    "    - **Relationship Strength:** \n",
    "        - The magnitude of the correlation coefficient (closer to 1 or -1) indicates the strength of the relationship.\n",
    "        - A value of 1 or -1 suggests a perfect linear relationship.\n",
    "    - **Correlation Matrix:** \n",
    "        - Displays the correlations between multiple variables, providing insights into the relationships among them.\n",
    "        - Here is an example of correlation matrix: \n",
    "        <div>\n",
    "        <img src=\"images/corr_matrix_CHSI.png\" width=\"400\"/>\n",
    "        </div>\n",
    "    - **Feature Selection:**\n",
    "        - Correlation analysis can be used in feature selection to identify and remove highly correlated features. \n",
    "        - Redundant features may not provide additional information and can lead to overfitting.\n",
    "    - **Model Interpretability:**\n",
    "        - Understanding the correlation between input features and the target variable can aid in model interpretability. \n",
    "        - Features with high correlation to the target may have stronger predictive power.\n",
    "    \n",
    "### 17. 1- What are the correlation limitations ? \n",
    "\n",
    "- It captures linear relationships but may not detect non-linear associations.\n",
    "- Additionally, correlation does not imply causation, and spurious correlations may exist.\n",
    "- For non-linear relationships or cases where data is not normally distributed, Spearman's rank correlation coefficient is an alternative measure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159139b8",
   "metadata": {},
   "source": [
    "## 18- What does cardinality mean in ML?\n",
    "- The term \"cardinality\" refers to the number of unique elements in a set or a feature.\n",
    "- It is a concept often used when dealing with categorical variables or discrete data.\n",
    "- Here are two common contexts in which cardinality is discussed:\n",
    "    - Feature Cardinality\n",
    "    - Set Cardinality\n",
    "### 18. 1- What is Feature Cardinality?\n",
    "- For categorical variables (nominal or ordinal), \n",
    "    - Cardinality represents the number of unique categories or levels within that variable.\n",
    "    - High cardinality: there are many distinct categories and it can be challenging for some machine learning algorithms:\n",
    "        - It may lead to increased dimensionality and computational complexity.\n",
    "        - It can also impact model interpretability.\n",
    "        \n",
    "### 18. 2- What is Set Cardinality?\n",
    "\n",
    "- In mathematical context, cardinality refers to the size or count of elements in a set.\n",
    "- In the context of machine learning, this could relate to the size of a dataset, the number of samples, or the number of unique instances.\n",
    "\n",
    "\n",
    "**Note:**\n",
    "- It's particularly relevant when dealing with categorical variables, where high cardinality can pose challenges that need to be addressed in the data preparation and model-building processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d7129",
   "metadata": {},
   "source": [
    "## 19- What is Variance? \n",
    "\n",
    "- Understanding variance is crucial in assessing the stability and generalization capability of models.\n",
    "- It refers to the degree of spread or dispersion in a set of values.\n",
    "- It measures the variability of each individual data points (observation) from the mean (average) of the dataset:\n",
    "    - Higher variance: \n",
    "        - Data points are more spread out from the mean\n",
    "        - More dispersed distribution\n",
    "        - A broader spread\n",
    "    - Lower variance:\n",
    "        - Data points are closer to the mean\n",
    "        - More concentrated distribution.\n",
    "        \n",
    "- Formula:  $$\\sigma^2 = { \\sum \\limits _{i=1} ^{n}(X_{i} - \\overline{X}) \\over {n-1}}$$\n",
    "- Where :\n",
    "    - $\\hat X$: the mean\n",
    "    - $n$: the number of data points\n",
    "    - $X_{i}$: represents each individual data point.\n",
    "    \n",
    "- The standard deviation ($\\sigma$) is the square root of the variance.\n",
    "- Understanding the variance of a model's predictions is essential. If the predictions variance is :\n",
    "    - Low: predictions varying little from each other. \n",
    "    - High: \n",
    "        - Can indicate overfitting\n",
    "        - Reading too deelpy into the noise, good performance on training data while poor performance on testing data\n",
    "\n",
    "**Note:**  \n",
    "- Outliers can have a significant impact on the variance, making it sensitive to extreme values.\n",
    "- Understanding and managing variance is crucial for building models that generalize well to new data and avoiding overfitting.\n",
    "- Do not forget the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebb0da2",
   "metadata": {},
   "source": [
    "## 20 - What is standard deviation ?\n",
    "\n",
    "- It is a statistical measure that quantifies the amount of variation or dispersion in a set of values.\n",
    "- It provides a way to understand how spread out or clustered data points are around the mean (average) of a dataset. - Denoted by the symbol σ (sigma) for a population and s for a sample.\n",
    "- Interpretation : if Standard Deviation value is \n",
    "    - High :\n",
    "        - Data points are more spread out from the mean.\n",
    "        - The values are dispersed over a wider range, indicating higher variability.\n",
    "    - Small: \n",
    "        - Data points in the dataset are close to the mean.\n",
    "        - The values are tightly clustered identifying low variability or dispersion.\n",
    "- Formula : $$ \\sigma= \\sqrt{ \\sum(x_{i}- \\mu)^2) \\over N}$$\n",
    "- Where : \n",
    "    - σ is the standard deviation.\n",
    "    - $x_{i}$ represents each data point.\n",
    "    - μ is the mean of the dataset.\n",
    "    - N is the total number of data points\n",
    "\n",
    "**Note:**\n",
    "- Standard deviation gives a general overview about the variability or consistency of data.\n",
    "- Can be used to compare two datasets : \n",
    "    - Smaller standard deviation: suggests that the values are more consistent\n",
    "    - Larger standard deviation: implies greater diversity or variability.\n",
    "    - In a normal distribution:\n",
    "        - About 68% of the data falls within one standard deviation of the mean.\n",
    "        - About 95% falls within two standard deviations.\n",
    "        - About 99.7% falls within three standard deviations.\n",
    "<div>\n",
    "<img src=\"images/sd2_orig.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad813e",
   "metadata": {},
   "source": [
    "## 21 - What does bias term mean?\n",
    "- In the context of machine learning, the term \"bias\" can refer to two different concepts:\n",
    "    - Data bias\n",
    "    - bias in the context of bias-variance tradeoff\n",
    "    - And bias term in the context of linear models. \n",
    "    \n",
    "### 21. 1-  What does data bias mean?\n",
    "- It is when the available data used in the training phase is not representative of the real-world population or phenomen of study.\n",
    "- It refers to the presence of systematic errors or inaccuracies in a dataset that can lead to unfair, unrepresentative, or skewed results when analyzing or modeling the data.\n",
    "- The existence of biased data can lead to undesired and often unfair outcomes (discriminatory results) when the model is applied to testing data because the model will learn these biases too. \n",
    "- A high-bias model makes strong assumptions about the underlying relationships in the data, and as a result, it may overlook complexity and fail to capture the true patterns.\n",
    "- Example: a biased facial recognition model may perform poorly for certain demographic groups.\n",
    "- Various types of bias are existing :\n",
    "    - Selection bias : when the process of selecting data points for the dataset is not random, leading to a non-representative sample\n",
    "    - Measurement bias : errors or inconsistencies in how data is measured or recorded. Examples: errors in sensors, discrepancies in data collection methods, or differences in measurement standards.\n",
    "    - Sampling Bias : if the method used to collect or sample data introduces a bias. \n",
    "    - Observer Bias: occurs when the individuals collecting or interpreting the data have subjective opinions or expectations that influence the data\n",
    "    - Historical Bias : when historical data includes biased decisions or reflects societal biases, machine learning models trained on such data may perpetuate or even exacerbate those biases.\n",
    "    - Algorithmic Bias : occurs when machine learning algorithms learn and perpetuate biases present in the training data. \n",
    "\n",
    "**Note:**\n",
    "    - Addressing data bias is an ongoing challenge in the field of machine learning, and researchers and practitioners are actively working to develop methods and tools to identify, measure, and mitigate bias in models.\n",
    "    \n",
    "### 21. 2- What does Bias-variance trade off mean?\n",
    "- It is a fundamental concept in machine learning that involves finding the right balance between two sources of error, namely bias and variance, when building predictive models.\n",
    "- The tradeoff arises because decreasing bias often increases variance, and vice versa. \n",
    "- Key points about the bias-variance tradeoff: \n",
    "    - **High bias : underfitting:** a model is too simple and it missunderstand the relevant relations between features and target outputs. It leads to systematic errors on both the training and test data.\n",
    "    - **High variance : overfitting:** a model is too complex and fits the training data too closely, capturing noise as if it were a real pattern. It perform poorly on new, unseen data. \n",
    "- The goal is to find the optimal model complexity that minimizes both bias and variance, resulting in good generalization to new data.\n",
    "\n",
    "![title](images/bias_variance_tradeoff.jpeg)  \n",
    "\n",
    "### a- How to find the right balance between variance and bias?\n",
    "- Cross-validation techniques, such as k-fold cross-validation, can be used to estimate a model's bias and variance and guide the selection of an appropriate model complexity.\n",
    "- Techniques like regularization can be employed to penalize overly complex models, helping to mitigate overfitting and find a better bias-variance tradeoff.\n",
    "\n",
    "###  b - What is Bias-Variance Decomposition? \n",
    "- It is a mathematical expression that breaks down the mean squared error (MSE) of a predictive model into three components: bias squared, variance, and irreducible error. \n",
    "- Formula : $$MSE=Bias^2 +Variance+Irreducible Error$$\n",
    "- Irreducible error is the inherent noise or randomness in the data that cannot be reduced by any model. It represents the minimum achievable error, even with a perfect model.\n",
    "- This decomposition provides insights into the sources of error in a model and helps to illustrate the bias-variance tradeoff.\n",
    "- We aim to find the optimal model complexity that minimizes the sum of bias squared and variance.\n",
    "\n",
    "**Note:**\n",
    "- Balancing bias and variance is a central challenge in machine learning, and understanding this tradeoff is essential for model selection, training, and evaluation. \n",
    "\n",
    "    \n",
    "###  c- How to mitigate Bias in ML?      \n",
    "- To mitigate bias it's crucial to accomplish well studied steps: \n",
    "    - Collecting diverse and representative data.\n",
    "    - Implement ethical data collection practices.\n",
    "    - Thoroughly processing it to detect and identify biases in the data. \n",
    "    - Regularly checking model predictions to ensure fairness and to detect and rectify biases in the model predictions.\n",
    "    - Develop and use algorithms that are designed to be aware of and mitigate biases : `Fairness-aware Algorithms`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad72003",
   "metadata": {},
   "source": [
    "### 21. 3- What is bias term in the context of linear models? \n",
    "- Also, it is known as the intercept or constant term.\n",
    "- It is a constant value added to the linear combination of input features.\n",
    "- In simple linear regression model:\n",
    "    - Equation is: $$y = mx + b$$\n",
    "    - Where : \n",
    "        - y is the predicted output.\n",
    "        - x is the input feature.\n",
    "        - m is the coefficient (weight) of the input feature.\n",
    "        - b is the bias term.\n",
    "    - Bias term is the y-intercept of the linear equation, indicating the value of y when x is zero.\n",
    "- In complex models with multiple features:\n",
    "    - Equation: $$Y=W \\times X + b$$\n",
    "    - Where:\n",
    "        - Y: is the vector of predicted outputs.\n",
    "        - X: is the matrix of input features.\n",
    "        - W: is the vector of weights (coefficients) for each feature.\n",
    "        - b: is the bias term vector.\n",
    "- Bias term provides flexibility and allows the model to make predictions ( even when all input features are zero) that are not strictly dependent on the input features alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eeda89",
   "metadata": {},
   "source": [
    "## 22- What does multicolinearity mean?\n",
    "- It is a statistical concept where several independent variables in model are correlated\n",
    "- If correlation coefficient is +/- 1 ==> those two variables \"perfectly collinear\".\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b709a78",
   "metadata": {},
   "source": [
    "## 23 - Hyperparameters tuning or hyperparameter optimization\n",
    "### 23. 1- What does hyperparameter mean?\n",
    "- Hyperparameters are external configuration settings that are not learned from the data but are set before the training process begins.\n",
    "- These settings influence the learning process and the overall behavior of the model.\n",
    "- Examples of hyperparameters :\n",
    "    - Learning rates\n",
    "    - Regularization parameters\n",
    "    - Hidden layers number\n",
    "    - Nodes number\n",
    "    - Decision tree depth\n",
    "- The choice of hyperparameters, which is called hyperparameter tuning can influence the performance of a machine learning model. \n",
    "- It is crucial to find the optimal values and achieve the best possible predictive performance.\n",
    "\n",
    "### 23. 2-  What is the difference between a parameter and an hyperparameter? (check)\n",
    "- Each machine learning model has : \n",
    "    - Parameters\n",
    "    - Hyperparameters\n",
    "- **Model parameters:**\n",
    "    - It is a configuration variables that is internal to the model.\n",
    "    - It is learned from the training data during the model training process and not set manually.\n",
    "    - It is required to the model to make prediction.\n",
    "    - Corresponds to the coefficients or weights associated with features in a model.\n",
    "    - They are crucial for defining the model's structure and capturing patterns in the training data.\n",
    "    - They are adjusted during the training process to minimize the difference between the model's predictions and the actual outcomes.\n",
    "    - Examples:\n",
    "        - $y=wx+b$ : w and b are parameters\n",
    "        - $y=ax^2+bx+c$ : a, b,c are parameters\n",
    "        - Support vectors in SVM \n",
    "        - In a neural network, the weights and biases connecting the neurons are parameters.\n",
    "- **Model hyperparameters:**\n",
    "    - An hyperparamter is an external configuration.\n",
    "    - It is set prior to the training process, hyperparameters tuning.\n",
    "    - Their selection is crucial for achieving optimal model performance:\n",
    "        - Can be achieved using an optimal solution:\n",
    "            - GridSearch \n",
    "            - RandomSearch\n",
    "            - Copy from previous problems\n",
    "        - Or they can be set manually\n",
    "    - Unlike parameters, hyperparameters are not learned from the data but are chosen based on prior knowledge, experimentation, or heuristics.\n",
    "    - They influence the overall behavior of the model and its learning process. \n",
    "    - Examples:\n",
    "        - Learning rate in NN or in gradient descent\n",
    "        - C and *sigma* in SVM\n",
    "        - Depth of a decision tree in a random forest\n",
    "        - K in KNN\n",
    "**Note:**\n",
    "- Adjusting parameters improves the model's fit to the training data, while selecting appropriate hyperparameter values affects the model's overall behavior and generalization performance.\n",
    "- Understanding the distinction between parameters and hyperparameters is crucial for effectively building and optimizing machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37601959",
   "metadata": {},
   "source": [
    "### 23. 3- What does hyperparameter tuning mean? \n",
    "- It is called hyperparameter optimization or model selection.\n",
    "- It corresponds to finding the best set of hyperparameters for a machine learning model.\n",
    "- Here are common steps of Hyperparameter tuning :\n",
    "    - Define a Search Space\n",
    "    - Choose a Search Method\n",
    "    - Choose the right Objective Function\n",
    "    - Search for Optimal Hyperparameters\n",
    "    - Evaluate Performance\n",
    "    - Select Best Hyperparameters\n",
    "    - Final Model Training\n",
    "- **Define a Search Space :** select the set of hyperparameters to be tuned and define a range of possible values for each.\n",
    "- **Choose a Search Method:** choose a Search Method : Grid Search, Random Search, and more advanced techniques like Bayesian optimization.\n",
    "- **Choose the right Objective Function:** select an objective function that evluates the performance of the model for a given set of hyperparameters. Examples: accuracy, precision, recall, or any other relevant measure.\n",
    "- **Select Best Hyperparameters:** it involves training and evaluating the model with various hyperparameter combinations. Then, choose the optimal values.\n",
    "\n",
    "- Hyperparameter tuning is essential for improving the generalization performance of a machine learning model.\n",
    "- It helps to avoid overfitting and ensures that the model is well-configured to handle new, unseen data effectively.\n",
    "\n",
    "### 23. 4- What is Grid Search? \n",
    "\n",
    "- Performed using `GridSearchCV` of `scikit-learn`.\n",
    "- It consists on performing an exhaustive search for selecting a model using a predefined hyperparameter grid.\n",
    "- The data scientist set up a grid of hyperparameters values and for each combination, trains a model and evaluate performance on testing data ==> to select, at the end, the optimal parameters.\n",
    "- It explores the entire search space by following a grid pattern. \n",
    "- The search space is defined by specifying discrete values or ranges for each hyperparameter\n",
    "- It is deep as it guarantees that every combination is evaluated.\n",
    "- However, it is computationally intensive especially when dealing with a large number of hyperparameters or a broad range of values.\n",
    "    \n",
    "### 23. 5- What is Random search?\n",
    "    \n",
    "- Set up a grid of hyperparameter values and selects random combinations to train the model and score.  \n",
    "- Method: Random search randomly samples a specified number of hyperparameter combinations from the defined search space.\n",
    "- Exploration: It explores the hyperparameter space randomly, which can be more efficient in some cases.\n",
    "- Search Space: The search space is defined similarly to grid search but does not require discretization; it can handle continuous and discrete hyperparameters.\n",
    "- Computational Efficiency: Random search is often more computationally efficient than grid search because it does not exhaustively evaluate every combination.\n",
    "    \n",
    "    \n",
    "### 23. 6- How to choose between Random Search and Grid Search  ?\n",
    "\n",
    "- Choosing between Random Search and Grid Search for hyperparameter tuning depends on:\n",
    "    - The specific characteristics of the machine learning model\n",
    "    - The size of the hyperparameter search space\n",
    "    - The computational resources\n",
    "- Here is some details regarding both of them :\n",
    "    - **Random Search :**\n",
    "        - Computationally Efficient: can reach optimanl combination with fewer iterations.\n",
    "        - Suitable for Large Search Space: hyperparameter search space is large, Random Search is preferred as it explores different combinations without exhaustively trying every combination.\n",
    "        - Initial Exploration: useful for an initial exploration of hyperparameter space when you have limited knowledge about which hyperparameters are critical.\n",
    "        - Resource Constraints: in case we have some computational resource constraints (limited time or computing power), Random Search can provide decent results in a shorter time compared to Grid Search.\n",
    "    - **Grid Search:** \n",
    "        - Exhaustive Search: performs an exhaustive search over all specified hyperparameter combinations. It systematically evaluates every combination in the predefined grid\n",
    "        - Smaller Search Spaces: it is suitable for smaller search spaces where trying every combination is feasible.\n",
    "        - Fine-Tuning: if you have some prior knowledge about the hyperparameter values that are likely to work well, Grid Search can be used for fine-tuning around those values.\n",
    "        - Interactions Between Hyperparameters: if there are interactions between hyperparameters, Grid Search may be better at capturing those interactions as it evaluates combinations systematically.\n",
    "    - **Hybrid Approach: both:**\n",
    "        - Start with Random Search to broadly explore the hyperparameter space and identify promising regions. Then, use Grid Search to perform a more focused search in those regions.\n",
    "        - The choice depends on the trade-off between computational resources and the desire for an exhaustive search.\n",
    "        - Random Search may provide good results with less computation, but Grid Search guarantees an exhaustive search. \n",
    "        \n",
    "\n",
    "**Note:**\n",
    "- It is common to try both methods and observe their impact on model performance and training time.\n",
    "- It is preferred to use cross-validation to evaluate the performance of different hyperparameter combinations and avoid overfitting to the training data.\n",
    "- For complex models or large datasets, Random Search is often preferred. For simpler models or smaller datasets, Grid Search may be feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd02eb",
   "metadata": {},
   "source": [
    "### 23. 7- What is the difference between paramter tuning, hyperparamter tuning and fine tuning?\n",
    "- In the context of optimization:\n",
    "    - Parameter tuning: involves finding the optimal values for the internal variables\n",
    "    - hyperparameter tuning: involves finding the best configuration for external settings.\n",
    "- Fine-tuning can refer to different processes depending on the context:\n",
    "    - **Model Fine-tuning:** \n",
    "        - In case of transfer learning (pre-trained models in DL), fine-tuning refers to adjusting a pre-trained model on a new, domain-specific dataset.\n",
    "        - The pre-trained model, often trained on a large dataset, serves as a starting point, and the model is further trained on the new dataset to adapt to the specific task at hand.\n",
    "    - **Algorithm Fine-tuning:** \n",
    "        - The adjustment or customization of algorithms to better fit a specific problem or dataset.\n",
    "        - It involves making small adjustments to the algorithm's parameters or characteristics to achieve better performance.\n",
    "\n",
    "**Note:**\n",
    "- Fine-tuning can have a broader interpretation, including adjusting pre-trained models or making small adjustments to algorithms for better alignment with specific tasks or datasets.\n",
    "- Fine-tuning may involve parameter tuning, but it can encompass a more comprehensive process of model adaptation or algorithm customization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526c8be",
   "metadata": {},
   "source": [
    "## 24- What does interpolation and extrapolation mean?\n",
    "- **Interpolation :** \n",
    "    - It is a mathematical and statistical technique used to estimate or predict values within the range of known data points.\n",
    "    - It is used to fill in the gaps between observed data points and provide estimates for values that fall within the known range.\n",
    "    - Example: if you have f(1), f(2) and f(3) then interpolation could be used to estimate the function's value at f(1.5).\n",
    "- **Extrapolation :**\n",
    "    - It corresponds to the process of estimating or predicting values outside the range of known data points.\n",
    "    - It is used to extend the trend or pattern observed in the existing data to make predictions beyond the observed range.\n",
    "    - It can be risky, especially when the data may exhibit behavior that deviates from the observed pattern.\n",
    "    - Example: Example: if you have f(1), f(2) and f(3) then extrapolation could be used to estimate the function's value at f(4).\n",
    "    \n",
    "**Note:**\n",
    "- Interpolation is generally considered more reliable and accurate because it involves estimating within the known range.\n",
    "- Extrapolation carries a higher risk of inaccuracy, especially if the underlying pattern in the data changes beyond the observed range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21936fcb",
   "metadata": {},
   "source": [
    "## 25- Correlation matrix versus Covariance matrix ? \n",
    "- Correlaion matrix:\n",
    "    - A correlation matrix is a table that shows the correlation coefficients between many variables.\n",
    "    - Correlation coefficients in the matrix:\n",
    "        - Are standardized, ranging from -1 to 1\n",
    "        - If coefficients >0 : we have positive correlation\n",
    "        - If coefficients <0 : we have negative correlation\n",
    "        - If coefficients =0 : indicates no correlation\n",
    "        - If coefficients =1 : we have a perfect positive correlation\n",
    "        - If coefficients =-1 : we have a perfect negative correlation\n",
    "    - It is a **standardized** form of covariance.\n",
    "    - It measures the linear relationship of variables.\n",
    "    - Correlation measures when a change in one variable can result a change in another.\n",
    "    - How strongly two random variables are related to each other.\n",
    "\n",
    "- Covariance matrix: \n",
    "    - Tells us the direction of the linear relationship between two random variables\n",
    "    - The covariance matrix is in the original units of the variables, making it **sensitive to changes in scale**.\n",
    "    - It is used to determine how much two random variables vary together.\n",
    "**Note:**\n",
    "- For covariance, interpreting the strength of the relationship is challenging due to the scale sensitivity.\n",
    "- In practice, correlation matrices are often preferred in data analysis and statistics because they provide standardized measures of association that are easier to interpret and compare.\n",
    "- Correlation is easier to interpret as it ranges between -1 and 1\n",
    "- Covariance is sensitive to scale and may not provide a clear indication of the strength of the relationship.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30096fd",
   "metadata": {},
   "source": [
    "## 26- What is Maximal Information Coefficient (MIC)? \n",
    "- It is used to measure the strength and complexity of the relationship between two variables.\n",
    "- It is designed to be a robust measure of association, capable of identifying various types of relationships between variables, including monotonic, non-monotonic, and complex dependencies.\n",
    "- MIC values range from 0 to 1, where 0 indicates no association, and 1 indicates a perfect association.\n",
    "- The MIC is computed by searching over grids of possible cutoffs to find the maximal value of the mutual information statistic.\n",
    "- It is often used in exploratory data analysis, feature selection, and identifying relevant variables in high-dimensional datasets\n",
    "- MIC is considered nonparametric because it does not assume a specific functional form for the relationship between variables\n",
    "\n",
    "**Note:**\n",
    "- While MIC is a powerful measure, it may not perform optimally in all scenarios, and its effectiveness can depend on the characteristics of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69f959",
   "metadata": {},
   "source": [
    "\n",
    "## 27- What is the difference between Type I error and Type II error ?\n",
    "- Type I error (False Positive):\n",
    "    - Occurs when the null hypothesis is true and we incorrectly reject it\n",
    "    - It represents the situation of False positive, where the test incorrectly concludes that there is an effect or difference when, in reality, there is none.\n",
    "    - Often denoted by α, the significance level, which is the probability of making a Type I error.\n",
    "    - Example: Concluding that a new drug is effective when it actually has no effect.\n",
    "- Type II error (False Negative):\n",
    "    - Occurs when the null hypothesis is false and we accept it. \n",
    "    - False negative something has happened and we are missing it.\n",
    "    - It represents the situation where the test fails to detect a real effect or difference when, in reality, there is one.\n",
    "    - Often denoted by β, which is the probability of making a Type II error\n",
    "    - 1−β represents the probability of correctly rejecting a false null hypothesis.\n",
    "    - Example: Failing to conclude that a new drug is effective when it actually has a positive effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffe5f1",
   "metadata": {},
   "source": [
    "## 28- What is Data leakage ? \n",
    "- It refers to the situation where information from outside the training dataset is used to create a model. \n",
    "- The training data contains Information about the target but similar data will not be available when the model is used for testing.\n",
    "- Data leakage can significantly impact the generalization ability of the model:\n",
    "    - High performance on Training set \n",
    "    - Perform poorly on new, unseen data (production).\n",
    "- There are two main types of data leakage:\n",
    "    - Train-Test Contamination\n",
    "    - Target Leakage\n",
    "\n",
    "### 28. 1- What is Train-Test Contamination?\n",
    "\n",
    "- This type of leakage occurs when information from the test set or validation set( any data that the model should not have access to during training) inadvertently influences the model training process.\n",
    "- For example, if you preprocess the entire dataset (including the test set) before splitting it into training and testing sets, and your preprocessing involves calculations or transformations based on information that should only be available in the future, then you have introduced data leakage.\n",
    "- Here is the optimal solution:\n",
    "    - Call train_test_split() to split the data into training, validation and testing\n",
    "    - Perform pre-processing such as normalization or standardisation\n",
    "    - Exculde validation data from any type of fitting, including the fitting of pre-processing steps.\n",
    "    - It is better to use Scikit-learn pipelines\n",
    "- It is recommended to split training and validation sets carefully so we can prevent Train-Test Contamination and pipelines can help implement this seperation\n",
    "\n",
    "### 28. 2- What is Target Leakage?\n",
    "\n",
    "- It happens when predictors include data that will not be available at the time you make predictions.\n",
    "- It often happens when we work with timing or chronological order (time series data)\n",
    "- For instance, if you use information in the training set that is derived from the target variable (the variable you are trying to predict) but would not be known at the time of prediction, it can lead to a model that performs well on the training data but poorly on new, unseen data.\n",
    "- Example : detection the existence of pneumonia, feature \"took anti-biotic\" does not help because it comes after gotting the disease.==> All not usuable variables, those which were created after the Target parameter should be excluded.\n",
    "\n",
    "### How to avoid data leakage? \n",
    "- Preventing data leakage is crucial for building reliable and generalizable machine learning models.\n",
    "- To avoid data leakage, it's important to:\n",
    "    - Strictly separate training and testing data\n",
    "    - Ensure that feature engineering is done using only information available at the time of prediction\n",
    "    - Be cautious about any transformations or preprocessing steps that might inadvertently introduce information from the future into the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7423b3",
   "metadata": {},
   "source": [
    "## 29- What does Instance-Based Learning means : \n",
    "- Also, it is known as instance-based reasoning or memory-based learning or lazy learning.\n",
    "- It is a type of machine learning approach that makes predictions based on the similarity between new instances and instances in the training dataset.\n",
    "- Instead of learning an explicit model during training, instance-based learning stores the entire training dataset and uses it to make predictions for new, unseen instances.\n",
    "- Examples: K-Nearest Neighbors (KNN), where predictions are based on the majority class or average of the k-nearest neighbors in the feature space.\n",
    "-  The model does not explicitly generalize from the training data; it memorizes instances and relies on similarity measures to make predictions for new, unseen instances.\n",
    "- It is suited for tasks where the relationships between input features and output labels are not easily captured by a simple model.\n",
    "- It can be robust in the presence of noise and is capable of handling complex decision boundaries.\n",
    "- However, it may be computationally expensive, especially when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e44f8f",
   "metadata": {},
   "source": [
    "## 30- What is the difference between Inductive ML and Deductive ML?\n",
    "- **Inductive Learning:**\n",
    "    - Inductive machine learning involves learning patterns, rules, or models from specific examples or instances.\n",
    "    - It generalizes from observed examples to make predictions or classifications for new, unseen instances.\n",
    "    - Example: In classification, an inductive ML algorithm learns to classify emails as spam or not spam based on a set of labeled examples.\n",
    "- **Deductive Learning:**\n",
    "    - Deductive machine learning involves deriving specific conclusions or predictions from general principles or rules.\n",
    "    - It starts with general rules or principles and applies them to specific instances to make predictions or decisions.\n",
    "    - Example: In expert systems, deductive reasoning is used to derive specific recommendations or diagnoses based on predefined rules and knowledge.\n",
    "    \n",
    "**Note:**\n",
    "- Inductive learning generalizes from specific examples to make predictions.\n",
    "- Deductive learning starts with general rules and applies them to make specific predictions or decisions.\n",
    "- Inductive learning is data-driven, while deductive learning is rule-driven."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b5457",
   "metadata": {},
   "source": [
    "## 31- Why should we create a ML pipeline?\n",
    "- Creating a Machine Learning (ML) pipeline offers several advantages and is considered a best practice in ML workflows.\n",
    "- Here are some key reasons why you should create an ML pipeline:\n",
    "    - **Reproducibility:** facilitate reproducibility of results by encapsulating the entire workflow, from data preprocessing to model training and evaluation, in a pipeline, you can recreate the same analysis easily.\n",
    "    - **Efficiency:** with ML process automation, we can increase efficiency, as tasks like data preprocessing, feature engineering, and model training are organized and executed in a systematic manner.\n",
    "    - **Consistency:** the application of data preprocessing steps and model training across different experiments or iterations can reduce the likelihood of errors caused by manual interventions.\n",
    "    - **Scalability:** as the size of the dataset or complexity of the model increases, the pipeline can be adapted to handle larger workloads without significant manual effort.\n",
    "    - **Model Deployment:** once a model is trained and evaluated, the same pipeline can be extended to include deployment tasks, making it seamless to transition from development to production.\n",
    "    - **Parameter Tuning:** by encapsulating model training within a pipeline, it becomes easier to conduct systematic grid search or random search for optimal hyperparameter values. Hyperparameter tuning becomes more manageable\n",
    "    - **Collaboration:** the standardized structure of pipelines allows team members to share and reproduce experiments, fostering collaboration and knowledge exchange.\n",
    "    - **Monitoring and Logging:** pipelines can be configured to include monitoring and logging at various stages. It is crucial for tracking the performance of models, understanding the impact of changes, and diagnosing issues in the workflow.\n",
    "    - **Adaptability:** if there areany  modifications to the dataset, changes in preprocessing steps, or updates to the model architecture, the pipeline can be easily adjusted without disrupting the entire workflow.\n",
    "\n",
    "![title](images/ml_pipeline.png) \n",
    "\n",
    "*Source: https://www.linkedin.com/pulse/overview-steps-machine-learning-pipeline-khalid-nazarov/\n",
    "\n",
    "**Note:**\n",
    "- ML pipelines provide a systematic and efficient approach to managing the end-to-end machine learning process, ensuring reproducibility, consistency, and scalability, while also facilitating collaboration and adaptation to evolving requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd244fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263480d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b01bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b51c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fecc791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb318d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e64e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
