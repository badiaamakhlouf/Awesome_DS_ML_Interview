{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20126c01",
   "metadata": {},
   "source": [
    "# Data Scientist Interview Questions\n",
    "## Part 3 : ML : modelling and evaluation\n",
    "\n",
    "This Jupyter notebook serves as a focused resource for individuals gearing up for technical interviews in the fields of machine learning engineering and data science. It specifically delves into questions related to all phases of machine learning model evaluation and deployment. Whether you're a candidate looking to sharpen your interview skills or an interviewer seeking insightful questions, this notebook provides valuable content for honing your understanding of machine learning evaluation and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27572c9",
   "metadata": {},
   "source": [
    "### 0- What does Machine Learning means ? \n",
    "Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to perform tasks without explicit programming. The core idea behind machine learning is to allow machines to learn patterns, make predictions, or optimize decisions based on data.\n",
    "\n",
    "Key concepts in machine learning include:\n",
    "- Types of Machine Learning: supervised, unsupervised and semi-supervised\n",
    "- Types of machine learning problems: classification, regression and clustering\n",
    "- Split data into Training, validation and testing sets (case of supervised)\n",
    "- Choose the right algorithm depends on the problem you want to solve\n",
    "- Model Evaluation\n",
    "- Hyperparameter Tuning\n",
    "- Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb4290",
   "metadata": {},
   "source": [
    "### 1- What are three stages of building a machine learning model ? \n",
    "- The process of building a machine learning model includes three main stages, These stages are:\n",
    "    - **Training phase:** after splitting the data into training and testing sets, training data is used to train our model on a labeled dataset. During the training phase, the model tries to learn relationships between input data and the corresponding output target values while adjusting its internal parameters. Throughout this phase, the model aims to maximise the accuracy of making precise predictions or classifications when exposed to unseen data.\n",
    "    - **Validation phase:** after the model is well trained, we evaluate it on a seperate dataset known as the validation set (maximum 10% of our data). This dataset is not used during the training process. Validation stage helps identify the existence of certain overfitting (model performing well on training data but poorly on new data) or certain underfitting (model needs more training to capture the underlying patterns in the data).\n",
    "    - **Testing (Inference) phase:** during this phase, the trained and validated model is applied to unseen dataset, called test dataset. This phase aims to evaluate the model's performance and provides a measure regarding the model's effectiveness and its ability to make accurate predictions in a production environment.\n",
    "    \n",
    "#### 1. 1- How to split your data while building a machine learning model ?    \n",
    "- During the model building phase, it is required to split the data into three main sets to evaluate the model's performance and effectiveness. The three sets are: \n",
    "    - Training: used to train the model and learn relationship between inputs and outputs, contains 70-80% of our total dataset\n",
    "    - Validation: used to validate the model, fine-tune the model's hyperparameters and assess its performance during training, it helps to prevent overfitting and underfitting. It contains 10-15% of the total data\n",
    "    - Testing: used to test and evaluate the model's performance against unseen data and after validation phase. It is used to measure how effective will our built model be in a production environment. It contains 10-15% of the total data.\n",
    "\n",
    "- Splitting data is accomplished after the preprocessing phase (handle missing values, categorical features, scale features, etc.). \n",
    "- It is important to ensure that the split is representative of the overall distribution of the data to avoid biased results.\n",
    "- It is favorable to use cross-validation technique. \n",
    "- No fixed rule to split data between training, validation and testing, portions can vary based on individual preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db499b",
   "metadata": {},
   "source": [
    "### 2- What are the types of ML algorithms ? \n",
    "Machine learning algorithms can be categorized into several types based on their learning styles and the nature of the task they are designed to solve.\n",
    "\n",
    "Here are some common types of machine learning algorithms:\n",
    "- **Supervised Learning** \n",
    "- **Unsupervised Learning**\n",
    "- **Semi-Supervised Learning**\n",
    "- **Deep Learning** \n",
    "- **Reinforcement Learning** \n",
    "- **Ensemble learning**  \n",
    "- **Ranking**\n",
    "- **Recommendation system** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47970ec3",
   "metadata": {},
   "source": [
    "#### 2.1 - What does supervised, unsupervised and semi-supervised means in ML? \n",
    "\n",
    "In machine learning, the terms \"supervised learning,\" \"unsupervised learning,\" and \"semi-supervised learning\" refer to different approaches based on the type of training data available and the learning task at hand:\n",
    "\n",
    "- **Supervised Learning :** training a model on a labeled dataset, where the algorithm learns the relationship between input features and corresponding target labels. Can be used for Regression (continous output) or Classification (discrete output). \n",
    "- **Unsupervised Learning :** Deals with unlabeled data and aims to find patterns, structures, or relationships within the data. Can be used for Clustering (Groups similar data points together) or association\n",
    "- **Semi-Supervised Learning:** Utilizes a combination of labeled and unlabeled data to improve learning performance, often in situations where obtaining labeled data is challenging and expensive.\n",
    "\n",
    "#### 2.2 - What are Unsupervised Learning techniques ?\n",
    " We have two techniques, Clustering and association: \n",
    " - Custering :  involves grouping similar data points together based on inherent patterns or similarities. Example: grouping customers with similar purchasing behavior for targeted marketing.. \n",
    " - Association : identifying patterns of associations between different variables or items. Example: e-commerse website suggest other items for you to buy based on prior purchases.\n",
    "#### 2.3 - What are Supervised Learning techniques ? \n",
    "We have two techniques: classfication and regression: \n",
    "- Regression : involves predicting a continuous output or numerical value based on input features. Examples : predicting house prices, temperature, stock prices etc.\n",
    "- Classification : is the task of assigning predefined labels or categories to input data. We have two types of classification algorithms: \n",
    "    - Binary classification (two classes). Example: identifying whether an email is spam or not.\n",
    "    - Multiclass classification (multiple classes). Example: classifying images of animals into different species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb48e0",
   "metadata": {},
   "source": [
    "### 3- Examples of well-known machine learning algorithms used to solve regression problems\n",
    "\n",
    "Certainly! Here are some well-known machine learning algorithms commonly used to solve regression problems:\n",
    "\n",
    "- Linear Regression\n",
    "- Lasso Regression\n",
    "- Ridge Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Gradient Boosting Algorithms (e.g., XGBoost, LightGBM)\n",
    "- Support Vector Machines (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Bayesian Regression\n",
    "- Neural Networks (Deep Learning):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b2972",
   "metadata": {},
   "source": [
    "### 4- Examples of well-known machine learning algorithms used to solve classification problems\n",
    "\n",
    "Certainly! Here are some well-known machine learning algorithms commonly used to solve classification problems:\n",
    "\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Support Vector Machines (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Naive Bayes\n",
    "- Neural Networks (Deep Learning)\n",
    "- AdaBoost\n",
    "- Gradient Boosting Machines (GBM)\n",
    "- XGBoost\n",
    "- CatBoost\n",
    "- LightGBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536354aa",
   "metadata": {},
   "source": [
    "### 5- Examples of well-known machine learning algorithms used to solve clustering problems\n",
    "\n",
    "Several well-known machine learning algorithms are commonly used for solving clustering problems. Here are some examples:\n",
    "\n",
    "- K-Means Clustering \n",
    "- Hierarchical Clustering\n",
    "- DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "- Mean Shift\n",
    "- Gaussian Mixture Model (GMM)\n",
    "- Agglomerative Clustering\n",
    "\n",
    "These algorithms address different types of clustering scenarios and have varying strengths depending on the nature of the data and the desired outcomes. The choice of clustering algorithm often depends on factors such as the shape of clusters, noise in the data, and the number of clusters expected.\n",
    "\n",
    "#### 5.1- K-Means \n",
    "- Most known and used clsutering algorithm\n",
    "- Has two version : Hard-Kmeans and Soft Kmeans\n",
    "- Steps of Hard- Kmeans:\n",
    "    - Choose number of clusters K\n",
    "    - start with initial guess : xk(0), k=1.....K\n",
    "    - etc.\n",
    "- Formula : y(n)=x(n)+v(n) :\n",
    "    - y(n): data point\n",
    "    - x(n): centroid \n",
    "    - v(n): Gausian noise (Gaussian, statistically idependent , mean=0 and ${\\sigma_{k}^2}$=variance\n",
    "- This algorithm has two disadvantages (problem with linear complexity):\n",
    "    - Number of clusters \n",
    "    - Random choice for the centroid : different choices can led to different results\n",
    "#### a- What soft Kmeans mean?\n",
    "- Does not require hard decision in which y(n) belongs to one and only one decision region\n",
    "- y(n) has different probabilityy that belongs to a cluster K\n",
    "- Centroid xk is calculated in function of weights assigned to each point\n",
    "#### b- What is the K-meadian\n",
    "- It is a clustering algorithm that uses the median to calculate the updated center/ centroid of group\n",
    "- why??? ==> median is less affected by outliers but this method is much slower for large datasets because sorting is required on each iteration to compute median\n",
    "\n",
    "#### 5.2- What is Hierarchical Clustering ? \n",
    "- It is a clustering technique used in data analysis and machine learning.\n",
    "- It starts with each data point as a separate cluster and then iteratively merges or splits clusters based on their similarity, forming a dendrogram.\n",
    "- Dendrogram is representation of clusters hierarchy, with the vertical lines indicating the merging or splitting points.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/Dendrogram.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "- Here are the key characteristics and steps used in hierarchical clustering:\n",
    "\n",
    "    1. Each data point starts as a singleton cluster\n",
    "    2. The similarity or dissimilarity between data points is calculated using a chosen distance metric. \n",
    "    3. Based on the similarity/dissimilarity values, the algorithm either merges clusters or splits data points into new clusters.\n",
    "    4. After merging or splitting clusters, a dendrogram is constructed.\n",
    "    5. The algorithm continues until all data points belong to a single cluster or until a predefined stopping criterion is met.\n",
    "\n",
    "- Hierarchical clustering can be classified into two main types:\n",
    "    - Agglomerative (Bottom-Up) Hierarchical Clustering: it starts with individual data points as separate clusters and merges them iteratively to form larger clusters.\n",
    "    - Divisive (Top-Down) Hierarchical Clustering: it starts with all data points in a single cluster and recursively splits them into smaller clusters.\n",
    "\n",
    "#### a. Advantages : \n",
    "- Does not require number of clusters in advance.\n",
    "- Easy to implement.\n",
    "- Produces a dendrogram which helps with understanding the data.\n",
    "- Intuitive visualization with dendrograms\n",
    "- It is possible to cut dendrogram if we want to change the number of clusters. \n",
    "- Captures the hierarchical structure of the data\n",
    "#### b. Disadvantages:\n",
    "- Computationally more intensive, especially for large datasets. \n",
    "- Need to identify distance between two observations or between two clusters\n",
    "- sometimes it is difficult to identify number of clusters. \n",
    "- Lack of flexibility when dealing with non-globular shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec82c9",
   "metadata": {},
   "source": [
    "#### 5.3. What distance metrics are used as similarity measures between two samples in clustering? \n",
    "- Several distance metrics are commonly used as similarity measures between two samples in data analysis and clustering.\n",
    "- The choice of distance metric depends on the nature of the data and the characteristics of the analysis.\n",
    "- Here are some commonly used distance metrics:\n",
    "    - Eculidian Distance \n",
    "    - Manhattan Distance \n",
    "    - Maximum Distance  \n",
    "    - Minkowski Distance \n",
    "    - Chebyshev Distance\n",
    "    - Hamming Distance\n",
    "    - Cosine Similarity\n",
    "    - Correlation Distance\n",
    "    - Jaccard Similarity Coefficient\n",
    "    \n",
    "Here are more details regarding each distance metric : \n",
    " - **Eculidian Distance:**\n",
    "     - Measures the straight-line distance between two points in Euclidean space.\n",
    "     - Suitable for continuous numerical data\n",
    "     - Formula: \n",
    "- **Manhattan Distance:**\n",
    "    - Calculates the sum of the absolute differences between the coordinates of two points.\n",
    "    - Suitable for data with attributes that have different units or scales.\n",
    "    - Formula: \n",
    "- **Minkowski Distance:** \n",
    "    - Generalizes both Euclidean and Manhattan distances.\n",
    "    - Parameterized by a parameter 'p,' where p = 1 corresponds to Manhattan distance, and p = 2 corresponds to Euclidean distance.\n",
    "    - Formula : \n",
    "- **Chebyshev Distance:**\n",
    "    - Measures the maximum absolute difference between coordinates.\n",
    "    - Suitable for data where outliers might have a significant impact.\n",
    "    - Formula: \n",
    "- **Hamming Distance:**\n",
    "    - Measures the number of positions at which corresponding symbols differ in two binary strings.\n",
    "    - Suitable for categorical data or binary data.\n",
    "    - Formula : \n",
    "- **Cosine Similarity:**\n",
    "    - Measures the cosine of the angle between two vectors.\n",
    "    - Suitable for text data, document clustering, and cases where the magnitude of the data points is less important.\n",
    "- **Correlation Distance:**\n",
    "    - Measures the similarity in shape between two vectors, taking into account the correlation between variables.\n",
    "    - Suitable for datasets where the relative changes in variables are more important than their absolute values.\n",
    "- **Jaccard Similarity Coefficient:**\n",
    "    - Measures the similarity between two sets by comparing the size of their intersection to the size of their union.\n",
    "    - Suitable for binary or categorical data.\n",
    "- **Mahalanobis Distance:**\n",
    "    - Takes into account the correlation between variables and the scales of the variables.\n",
    "    - Suitable for datasets with correlated variables and different variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5110948",
   "metadata": {},
   "source": [
    "#### d. How to calculate distance between two clusters ?\n",
    "- The distance between two clusters, often referred to as linkage or proximity.\n",
    "- These distance measures are used during the agglomeration process in hierarchical clustering.\n",
    "- The algorithm iteratively merges clusters based on the chosen linkage method until all data points belong to a single cluster.\n",
    "- The choice of linkage method can impact the resulting dendrogram and the interpretation of the cluster structure.\n",
    "- Different linkage methods may be suitable for different types of data and clustering objectives.\n",
    "- Here are some commonly used linkage methods:\n",
    "    - Single Linkage (Minimum Linkage)\n",
    "    - Complete Linkage (Maximum Linkage)\n",
    "    - Average Linkage\n",
    "    - Centroid Linkage (UPGMA - Unweighted Pair Group Method with Arithmetic Mean)\n",
    "    - Ward's Method\n",
    "\n",
    "Here are more details regarding each method:\n",
    "- **Single Linkage (Minimum Linkage):**\n",
    "    - The distance between two clusters is the minimum distance between any two points in the two clusters.\n",
    "    - Formula: $d(C_{1},C_{2})={min_{i \\in C_{1}, j \\in C_{2}} distance(i,j)}$\n",
    "   \n",
    "- **Complete Linkage (Maximum Linkage):**\n",
    "    - The distance between two clusters is the maximum distance between any two points in the two clusters.\n",
    "    - Formula: $d(C_{1},C_{2})={max{i \\in C_{1}, j \\in C_{2}} distance(i,j)}$ \n",
    "- **Average Linkage:**\n",
    "    - The distance between two clusters is the average distance between all pairs of points from the two clusters.\n",
    "    - Formula: $d(C_{1},C_{2})={1 \\over |C_{1}|\\times|C_{2}|}{\\sum \\limits _{i\\in C_{1}} \\sum \\limits _{j\\in C_{2}} distance(i,j)}$\n",
    "- **Centroid Linkage (UPGMA - Unweighted Pair Group Method with Arithmetic Mean):**\n",
    "    - The distance between two clusters is the distance between their centroids (mean vectors).\n",
    "    - Formula: d($C_{1}$,$C_{2}$)=distance(centroid($C_{1}$), centroid($C_{2}$))\n",
    "- **Ward's Method:**\n",
    "    - Minimizes the increase in variance within the clusters when merging.\n",
    "    - It considers the sum of squared differences within each cluster and the sum of squared differences between the centroids of the clusters.\n",
    "    - Formula: $d(C_{1},C_{2})= {\\sqrt{{|C_{1}|\\times|C_{2}|}\\over {|C_{1}|+|C_{2}|}}}distance(centroid(C_{1}), centroid(C_{2}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce66ed5",
   "metadata": {},
   "source": [
    "### 6 -What is Ensemble learning?\n",
    "\n",
    "Ensemble learning is a machine learning technique that involves combining the predictions of multiple individual models to improve overall performance and accuracy. Instead of relying on a single model, ensemble methods leverage the strengths of diverse models to compensate for each other's weaknesses. The idea is that by aggregating the predictions of multiple models, the ensemble can achieve better generalization and make more robust predictions than any individual model.\n",
    "\n",
    "There are several ensemble learning methods, with two primary types being:\n",
    "- **Bagging (Bootstrap Aggregating) :** \n",
    "    - Involves training multiple instances of the same model on different subsets of the training data, typically sampled with replacement. \n",
    "    - Examples : Random Forest, Bagged Decision Trees, Bagged SVM (Support Vector Machines), Bagged K-Nearest Neighbors, Bagged Neural Networks\n",
    "- **Boosting :**\n",
    "    - Focuses on sequentially training models, with each subsequent model giving more attention to the instances that the previous models misclassified. \n",
    "    - Examples: AdaBoost (Adaptive Boosting), Gradient Boosting, XGBoost (Extreme Gradient Boosting), LightGBM (Light Gradient Boosting Machine), CatBoost, GBM (Gradient Boosting Machine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176b31a",
   "metadata": {},
   "source": [
    "### 8- What is Recommender Systems\n",
    "Also known as recommendation systems or engines, are applications or algorithms designed to suggest items or content to users based on their preferences and behavior. These systems leverage data about users and items to make personalized recommendations, aiming to enhance user experience and satisfaction. There are two main types of recommender systems:\n",
    "\n",
    "- Content-Based Recommender Systems\n",
    "- Collaborative Filtering Recommender Systems\n",
    "Recommender systems are widely used in various industries, including e-commerce, streaming services, social media, and more. They help users discover new items, increase user engagement, and contribute to business success by promoting relevant content and products\n",
    "\n",
    "#### 8. 1- What is Content-Based Recommender Systems ? \n",
    "#### 8. 2- What is Collaborative Filtering Recommender Systems ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b831fc9",
   "metadata": {},
   "source": [
    "### 9 -What is Reinforcement Learning?\n",
    "\n",
    "Is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, allowing it to learn optimal strategies over time to maximize cumulative rewards. It is inspired by the way humans and animals learn from trial and error.\n",
    "\n",
    "Here are some applications of Reinforcement Learning : \n",
    "- Automated Robots\n",
    "- Natural Language Processing\n",
    "- Marketing and Advertising \n",
    "- Image Processing\n",
    "- Recommendation Systems\n",
    "- Traffic Control \n",
    "- Healthcare \n",
    "- Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba017a",
   "metadata": {},
   "source": [
    "### 10- What is Ranking ? \n",
    "\n",
    "Ranking in machine learning refers to the process of assigning a meaningful order or ranking to a set of items based on their relevance or importance. This is often used in scenarios where the goal is to prioritize or sort items based on their predicted or observed characteristics.\n",
    "\n",
    "Ranking problems are common in various applications, including information retrieval, recommendation systems, and search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece0440",
   "metadata": {},
   "source": [
    "### 11- Overfitting and Underfitting\n",
    "\n",
    "Overfitting and underfitting are common challenges in machine learning that relate to the performance of a model on unseen data.\n",
    "- Overfitting : occurs when a machine learning model learns the training data too well, capturing noise and random fluctuations in addition to the underlying patterns (as concept). High error on testing dataset\n",
    "\n",
    "- Underfitting : happens when a model is too simplistic and cannot capture the underlying patterns in the training data. High error rate on both training and testing datasets.\n",
    "\n",
    "#### 11. 1 - Overfitting Causes and Mitigation:\n",
    "- Causes:\n",
    "    - Too many features or parameters.\n",
    "    - Complex model architectures.\n",
    "    - Limited training data.\n",
    "- Consequences\n",
    "- Mitigation:\n",
    "    - Regularization techniques (e.g., L1 or L2 regularization).\n",
    "    - Feature selection or dimensionality reduction.\n",
    "    - Increasing the amount of training data.\n",
    "    - Using simpler model architectures.==> Less variables and parameters so variance can be reduced.\n",
    "    - Use of Cross-validation method \n",
    "    \n",
    "#### 11. 2 - Underfitting Causes and Mitigation:\n",
    "- Causes:\n",
    "    - Too few features or parameters.\n",
    "    - Insufficient model complexity.\n",
    "    - Inadequate training time or data.\n",
    "- Mitigation:\n",
    "    - Increasing the complexity of the model.\n",
    "    - Adding relevant features.\n",
    "    - Training for a longer duration.\n",
    "    - Considering more sophisticated model architectures.\n",
    "\n",
    "Achieving a balance between overfitting and underfitting is crucial. This balance, often referred to as the model's \"sweet spot,\" results in a model that generalizes well to new, unseen data. Techniques like cross-validation, hyperparameter tuning, and monitoring learning curves can help strike this balance during the model development process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b8142",
   "metadata": {},
   "source": [
    "### 12- What are the types of Regularization in Machine Learning\n",
    "\n",
    "- Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the objective/loss function.\n",
    "- It consists on adding a cost term that penalize the large weights of model.\n",
    "- There are mainly two types of regularization commonly used: L1 regularization (Lasso) and L2 regularization (Ridge). - Additionally, Elastic Net is a combination of both L1 and L2 regularization. \n",
    "\n",
    "Here are all the used techniques in ML : \n",
    "- L1 Regularization (Lasso)\n",
    "- L2 Regularization (Ridge)\n",
    "- Elastic Net\n",
    "\n",
    "#### 12. 1 - L1 Regularization (Lasso) : \n",
    "L1 regularization tends to shrink some coefficients exactly to zero, effectively excluding the corresponding features from the model. It is often used when there is a belief that some features are irrelevant. The penalty term is the sum of the absolute values of the regression coefficients.\n",
    "\n",
    "#### 12. 2 - L2 Regularization (Ridge) : \n",
    "\n",
    "L2 regularization tends to shrink coefficients toward zero without eliminating them entirely. It is effective in dealing with multicollinearity (high correlation between predictors) and preventing overfitting. The penalty term is the sum of the squared values of the regression coefficients.\n",
    "\n",
    "\n",
    "#### 12. 3 - Elastic Net: \n",
    "\n",
    "Elastic Net combines both L1 and L2 penalties in the objective function. It has two control parameters, alpha (which controls the overall strength of regularization) and the mixing parameter, which determines the ratio between L1 and L2 penalties. It is useful when there are many correlated features, and it provides a balance between Lasso and Ridge.\n",
    "\n",
    "\n",
    "These regularization techniques help improve the generalization performance of machine learning models by preventing them from becoming too complex and fitting noise in the training data. The choice between L1, L2, or Elastic Net depends on the specific characteristics of the dataset and the modeling goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b70ac",
   "metadata": {},
   "source": [
    "### 13- What is Model Validation Technique?\n",
    "\n",
    "Validation techniques in machine learning are essential for assessing the performance of a model and ensuring its ability to generalize well to unseen data. \n",
    "\n",
    "Here are some common validation techniques:\n",
    "- Train-Test Split \n",
    "- K-Fold Cross-Validation \n",
    "- Stratified K-Fold Cross-Validation\n",
    "- Leave-One-Out Cross-Validation (LOOCV)\n",
    "- Holdout Validation\n",
    "- Time Series Cross-Validation\n",
    "\n",
    "#### 13. 1 - What is train-test-validation split?\n",
    "- It is an important step to indicate how well a model will perform with real-world, unseen data.\n",
    "- A good train-test-validation split helps mitigate overfitting and ensures that evaluation is not biased.\n",
    "- It consists on dividing input dataset into three subsets:\n",
    "    - Training: 70-80% of the data\n",
    "    - Validation: 10-15% of the data\n",
    "    - Testing: 10-15% of the data\n",
    "- This split aims to ensure that the model is trained on a sufficiently large dataset, validated on a separate set to fine-tune parameters, and tested on a completely independent set to provide an unbiased evaluation of its performance.\n",
    "#### 13. 2 - What is K-Fold Cross-Validation?\n",
    "- It is a technique used to assess the performance and generalization ability of a model. \n",
    "- The input dataset will be divided into k equally sized folds/groups.\n",
    "- (K-1) folds are used for training and one fold is used for testing. Then, we evaluate the model. \n",
    "- Repeating the training and evaluation K times.\n",
    "- Each time a different fold is taken as the test set while the remaining data is used for training.\n",
    "- Here are the steps of the process :\n",
    "    - Data Splitting\n",
    "    - Model Training and Evaluation : iteration\n",
    "    - Performance Metrics : error, accuracy, recall, precision etc is evaluated for each iteration.\n",
    "    - Average Performance : average performance (error) is evaluated across all K iterations ==> provide a more reliable estimate of the model's performance.\n",
    "- Error formula : $e(n)={y(n)-\\hat y(n)}$ is calculated for each iteration where $\\hat y$ is the predicted value.\n",
    "- Ideally, K is 5 or 10. The optimal value may depend on the size and nature of the dataset.\n",
    "- A higher K value can result in a more reliable performance estimate but may increase computational costs.\n",
    "- K-fold is very helpful to limit issues related to the variability of a single train-test split.==> It provides a more robust evaluation of a model's performance by ensuring that every data point is used for testing exactly once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598bc8b",
   "metadata": {},
   "source": [
    "#### 13. 3 - What is Stratified K-Fold Cross-Validation? \n",
    "- It is an extension of K-Fold Cross-Validation that ensures the distribution of the target variable's classes is approximately the same in each fold as it is in the entire dataset.\n",
    "- In case of imbalanced datasets, this technique is prefered because some classes may be underrepresented.\n",
    "- It helps in addressing issues related to  overrepresented or underrepresented classes in specific folds, which could lead to biased model evaluations.\n",
    "- Here are the main steps for Stratified K-Fold Cross-Validation :\n",
    "    - Data Splitting : ensuring that each fold has an equal distribution for each class samples.\n",
    "    - Model Training and Evaluation: the same K-fold cross-validation, steps repeated K times\n",
    "    - Average Performance : the average performance is calculated at the end of all K iterations to provide a robust performance estimate.\n",
    "#### 13. 4 - What is Leave-One-Out Cross-Validation (LOOCV)?\n",
    "- It is a specific case of k-fold cross-validation where the number of folds (K) is set equal to the number of data points in the dataset. \n",
    "- Each iteration one point is dedicated to testing while the remaining samples are dedicated for training\n",
    "- The same as k-fold, we calculate the performance metric for each iteration then we evaluate the average.\n",
    "- The process is repeated until each data point has been used as a test set exactly once.\n",
    "- It has the next advantages: \n",
    "    - It minimizes bias introduced by the choice of a specific train-test split.\n",
    "    - It provides a robust estimate of a model's performance since each data point serves as both training and test data.\n",
    "    - It is computationally expensive, especially for large datasets, as we are going to create a model for each sample.\n",
    "    - It is preferably to be used with only small datasets.\n",
    "#### 13. 5 - What is Holdout Validation ?\n",
    "- It is known as a train-test split. \n",
    "- The input dataset will be divided into two subsets: a training set (70-80%) and a testing set (20-30%).\n",
    "- The exact split ratio depends on factors such as the size of the dataset and the nature of the machine learning task.\n",
    "- The testing set is called Holdout Set also and it helps gathering an initial estimate of a model's performance.\n",
    "- The performance metrics are accuracy, precision, recall, error, etc\n",
    "- This technique is suitable if the input dataset is large enough to provide sufficient data for both training and testing, and when computational resources are limited compared to more computationally intensive methods like cross-validation.\n",
    "- This technique could be not too reliable as the model performance can be influenced by the specific random split of data into training and testing sets. \n",
    "- To address this variability, multiple iterations of the holdout process can be performed, and the results can be averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba38c7",
   "metadata": {},
   "source": [
    "### 14- How to evaluate a Classification model?\n",
    "\n",
    "- Many metrics are commonly used to evaluate the performance of classification models in machine learning.\n",
    "- The choice of metrics depends on the specific goals and characteristics of the classification problem.\n",
    "- Here are some classification metrics:\n",
    "    - Confusion matrix\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - F1 Score\n",
    "    - Recall (Sensitivity or True Positive Rate)\n",
    "    - Specificity or True Negative Rate\n",
    "    - Area Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC)\n",
    "    - Area Under the Precision-Recall Curve (AUC-PR) \n",
    "- The choice of metrics depends on the specific requirements of the classification problem (binary classification or multiclass classification).\n",
    "- For example, in imbalanced datasets, where one class significantly has large number of samples than the second class, precision, recall, and F1 score are often more informative than accuracy.\n",
    "\n",
    "#### 14. 1- What is confusion matrix in classification problems?\n",
    "\n",
    "- Confusion matrix is a table used to measure the performance of classification model\n",
    "- It gives more details regarding the number of instances that were correctly or incorrectly classified for each class.\n",
    "- The confusion matrix is a valuable tool for assessing the strengths and weaknesses of a classification model and guiding further optimization efforts.\n",
    "- Here is an example of confusion matrix for a binary classification problem : \n",
    "![title](images/confusion-matrix1.jpeg)\n",
    "##### a. True Positive : \n",
    "- samples that are from the positive class and were correctly classified or predicted as positive by the model.\n",
    "##### b. True Negative :  \n",
    "- samples that are from the negative class and were correctly classified or predicted as negative by the model.\n",
    "##### c. False Positive : \n",
    "- samples that are from  the negative class but were incorrectly classified or predicted as positive by the model.\n",
    "##### d. False Negative : \n",
    "- samples that are from  the positive class but were incorrectly classified or predicted as negative by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569803d8",
   "metadata": {},
   "source": [
    "#### 14. 2- How to define Accuracy?\n",
    "\n",
    "- An evaluation metric used to evaluate the performance of classification model.\n",
    "\n",
    "- Divides the number of correctly classified observations by the total number of samples.\n",
    "\n",
    "- **Formula:** $$Accuracy ={ Number  of Correct Predictions \\over Total number of predictions }$$\n",
    "\n",
    "\n",
    "- Here a second formula : $$Accuracy ={ TP + TN \\over TP + TN + FP + FN }$$\n",
    "\n",
    "#### 14. 3- How to define Precision ?\n",
    "- An evaluation metric that measures the accuracy of the positive predictions made by the model. \n",
    "- It divides the number of true positive predictions by the sum of true positives and false positives.\n",
    "- It belongs to [0,1] interval, 0 corresponds to no precision and 1 corresponds to perfect precision.\n",
    "- Precision = Positive Predictive Power\n",
    "- **Formula:** $$Precision = {True Positives \\over True Positives + False Positives}$$ \n",
    "\n",
    "#### 14. 4- How to define Recall, Sensitivity or True Positive Rate?\n",
    "- An evaluation metric that measures the ability of the model to capture all the positive samples.\n",
    "- It divides number of true positives samples by the sum of true positives and false negatives.\n",
    "- Recall = Sensitivity = True Positive Rate. \n",
    "- **Formula:** $$ Recall= {True Positives \\over True Positives + False Negatives}$$\n",
    "#### 14. 5- How to define F1-score? \n",
    "- An evaluation metric that combines both Precision and Recall.\n",
    "- Wighted average of Precision and Recall.\n",
    "- It can be calculated using the `f1_score()` function of `scikit-learn`\n",
    "- F1 belongs to [0,1]: 0 is the worst case and 1 is the best.\n",
    "- **Formula :** $$F1= {2×Precision×Recall \\over Precision+Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfee771",
   "metadata": {},
   "source": [
    "#### 14. 5- How to define Specificity or True Negative Rate ?\n",
    "- Specificity measures the ability of the model to correctly identify negative instances.\n",
    "- It divides the true negatives samples by the sum of true negatives observations and false positives observations.\n",
    "- True Negative Rate = Specificity\n",
    "- **Formula:** $$Specificity={True Negatives \\over True Negatives + False Positives}$$ \n",
    "#### 14. 6- What is Receiver Operating Characteristic (ROC) and Area under-ROC curve (AUC-ROC)?\n",
    "- ROC curve is a graphical representation of the model's performance across different classification thresholds.\n",
    "- The shape of the curve contains a lot of information\n",
    "- Area under the ROC curve : AUC-ROC provides a single metric indicating the model's ability to distinguish between classes.\n",
    "- Here is ROC and AUC-ROC illustration:\n",
    "\n",
    "![title](images/roc-curve-original.png)\n",
    "\n",
    "- If AUC-ROC is high, then we have better model. Else, we have poor model performance.\n",
    "- Smaller values on the x-axis of the curve point out lower false positives and higher true negatives.\n",
    "- Larger values on the y-axis of the plot indicate higher true positives and lower false negatives.\n",
    "- We can plot the ROC curve using the `roc_curve()` scikit-learn function.\n",
    "- To calculate the accuracy, we use `roc_auc_score()` function of `scikit-learn`.\n",
    "* Note: False Positive Rate = 1- Specificity\n",
    "\n",
    "\n",
    "\n",
    "*source: https://sefiks.com/2020/12/10/a-gentle-introduction-to-roc-curve-and-auc/\n",
    "\n",
    "#### 14. 7- What is Area Under the Precision-Recall Curve (AUC-PR)?\n",
    "- Similar to AUC-ROC, AUC-PR represents the area under the precision-recall curve.\n",
    "- It provides a summary measure of a model's performance across various levels of precision and recall.\n",
    "- It can be calculated using the `precision_recall_curve()` function of `scikit-learn`.\n",
    "- The area under the precision-recall curve can be calculated using the `auc()` function of `scikit-learn` taking the recall and precision as input.\n",
    "\n",
    "![title](images/precision_recall_curve.png)\n",
    "\n",
    "*source: https://analyticsindiamag.com/complete-guide-to-understanding-precision-and-recall-curves/\n",
    "\n",
    "- The same here if AUC-PR is high, then we have better model. Else, we have poor model performance.\n",
    "- The recall is provided as the x-axis and precision is provided as the y-axis.\n",
    "#### a. When to Use ROC vs. Precision-Recall Curves?\n",
    "- Choosing either the ROC curves or precision-recall curves depends on your data distribution:\n",
    "    - ROC curves: preferable to be used when there are roughly equal numbers of observations for each class.\n",
    "    - ROC curves provide a good picture of the model when the dataset has large class imbalance.\n",
    "    - Precision-Recall curves should be used when there is a moderate to large class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e658d",
   "metadata": {},
   "source": [
    "#### 14. 8 - Classification Report Scikit-learn? \n",
    "- The `classification_report` function of `scikit-learn` provides a detailed summary of classification metrics for each class in a classification problem. \n",
    "- The report contains the next metrics:\n",
    "    - Precision\n",
    "    - Recall- sensitivity\n",
    "    - F1-score\n",
    "    - Specificity\n",
    "    - Support\n",
    "- Support: the number of actual instances of each class in the dataset.\n",
    "#### 14. 9- How do we evaluate a classification report?\n",
    "- High recall + high precision ==> the class is perfectly handled by the model. \n",
    "- Low recall + high precision ==> the model can not detect the class well but is highly trustable when it does.\n",
    "- High recall + low precision ==> the class is well detected but model also includes points of other class in it. \n",
    "- Low recall + low precision ==> class is poorly handled by the model\n",
    "#### 14. 10 What is log loss fucntion?\n",
    "- It is an evaluation metric used in logistic regression\n",
    "- Called logistic regression loss or cross-entropy loss\n",
    "- Input of this loss function is probability value that belongs to [0,1].\n",
    "- It measures the uncertaintly of our prediction based on how much it varies from the actual label.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd3844",
   "metadata": {},
   "source": [
    "### 15- What are the performance metrics for Regression? \n",
    "- Several performance metrics are commonly used to evaluate the accuracy and goodness of fit of regression models.\n",
    "- Here are some common performance metrics for regression:\n",
    "    - **Mean Absolute Error (MAE)**\n",
    "    - **Mean Squared Error (MSE)**\n",
    "    - **Root Mean Squared Error (RMSE)**\n",
    "    - **Mean Absolute Percentage Error (MAPE)**\n",
    "    - **R-squared (R2)**\n",
    "- The choice of metric is related to several goals and characteristics of the regression problem to solve.\n",
    "- It is possible to use one of the above metrics next to accuracy, precision, and the ability to explain variance.\n",
    "- Considering multiple metrics is better solution to gain a comprehensive understanding about the model performance.\n",
    "- Almost, all regression tasks uses error to evaluate the model: if error is high ==> we need either to change the model or retrain it with more data.\n",
    "\n",
    "#### 15. 1- What is Mean Absolute Error (MAE) ? \n",
    "\n",
    "- As its name indicates, it represents the average absolute difference between the predicted values and the actual values.\n",
    "- **Formula :** $$MAE = {1\\over n} {\\sum \\limits _{i=1} ^{n}|y_{i}-\\hat{y}_{i}|}$$\n",
    "\n",
    "#### 15. 2- What is Mean Squared Error (MSE) ?\n",
    "- It represents the average squared difference between the predicted values and the actual values.\n",
    "- It penalizes larger errors more heavily than MAE.\n",
    "- **Formula:** $$MSE = {1\\over n} {\\sum \\limits _{i=1} ^{n}(y_{i}-\\hat{y}_{i})^2}$$ \n",
    "#### 15. 3- What is Root Mean Squared Error (RMSE) ? \n",
    "- It represents the square root of the MSE\n",
    "- It provides a measure of the average magnitude of errors in the same units as the target variable.\n",
    "- **Formula:** $$RMSE= {\\sqrt MSE} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5b6dd",
   "metadata": {},
   "source": [
    "#### 15. 4- What is Mean Absolute Percentage Error (MAPE) ? \n",
    "- It calculates the average percentage difference between the predicted and actual values.\n",
    "- It is a relative error metric\n",
    "- **Formula:** $$MAPE={1\\over n} {\\sum \\limits _{i=1} ^{n}({|y_{i}-\\hat{y}_{i}| \\over |y_{i}|})} \\times 100$$\n",
    "#### 15. 5- What is R-squared (R2)\n",
    "- It measures the proportion of the variance in the target variable that is predictable from the independent variables.\n",
    "- It represents the correlation between true value and predicted value\n",
    "- **Formula:** $$ R^2= 1 - {MSE \\over Var(y) }$$\n",
    "- $$ R^2= 1- {{\\sum \\limits _{i=1} ^{n}(y_{i}-\\hat{y}_{i})^2} \\over {\\sum \\limits _{i=1} ^{n}(y_{i}-\\overline{y})^2}}$$\n",
    "- $\\overline{y}$: is the mean of the target variable\n",
    "- It is possible to use **Adjusted R-squared**, which provides a penalized version of R-squared that adjusts the model complexity\n",
    "#### a. Correlation :\n",
    "- It is a measure of linear relationship between two quantitative variables. \n",
    "- It belongs to [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddabf61",
   "metadata": {},
   "source": [
    "### 16. how to choose a classifier based on training dataset size?\n",
    "- If training set is small ==> it is better to use simple model with high bias and low variance seems to work better because they are less likely to overfit. \n",
    "- If training set is large ==> it is better to use model with low bias and high variance as this model type will tend to perform better with complex relationships. Example: Naive Bayes.\n",
    "-  Balancing variance and bias is essential for developing models that perform well on both training and unseen data.\n",
    "\n",
    "#### 16. 1-  What is data bias ?\n",
    "- It is when the available data used in the training phase is not representative of the real-world population or phenomen of study.\n",
    "- For example: when training data used to create a ml model has unfair discrepancies or inaccuracies. \n",
    "- The information provided by the data does not truly represent the situation.\n",
    "- The existence of biased data can lead to undesired and often unfair outcomes (discriminatory results) when the model is applied to testing data because the model will learn these biases too. \n",
    "- Various types of bias are existing : selection bias, measurement bias and confirmation bias.\n",
    "- Addressing data bias is an ongoing challenge in the field of machine learning, and researchers and practitioners are actively working to develop methods and tools to identify, measure, and mitigate bias in models.\n",
    "- To mitigate data bias in machine learning, it's crucial to accomplish well studied steps: collecting diverse and representative data, thoroughly processing it, and regularly checking model predictions to ensure fairness.\n",
    "- Example: a biased facial recognition model may perform poorly for certain demographic groups.\n",
    "\n",
    "#### 16. 2-  What is variance? \n",
    "\n",
    "- Understanding variance is crucial in assessing the stability and generalization capability of models.\n",
    "- It refers to the degree of spread or dispersion in a set of values.\n",
    "- It measures the variability of each individual data points (observation) from the mean (average) of the dataset:\n",
    "    - Higher variance: data points are more spread out from the mean ==> more dispersed distribution.\n",
    "    - Lower variance:  data points are closer to the mean ==> more concentrated distribution.\n",
    "- Formula:  $\\sigma^2 = { \\sum \\limits _{i=1} ^{n}(X_{i} - \\overline{X}) \\over {n-1}}$\n",
    "- The standard deviation ( $\\sigma$) is the square root of the variance.\n",
    "- If the predictions variance is :\n",
    "    - Low: predictions varying little from each other. \n",
    "    - High: overfitting + reading too deelpy into the noise+ good performance on training data +poor performance on testing data\n",
    "- Do not forget the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abba343",
   "metadata": {},
   "source": [
    "### 17- What are the performance metrics for Clustering ?\n",
    "- Evaluating the performance of clustering algorithms is less straightforward compared to supervised learning tasks like classification.\n",
    "- Clustering is often exploratory, and there may not be explicit labels for assessing correctness.\n",
    "- Several metrics and methods are commonly used to assess the quality of clustering results. Here are some performance metrics for clustering:\n",
    "    - Silhouette Score\n",
    "    - Davies-Bouldin Index\n",
    "    - Calinski-Harabasz Index (Variance Ratio Criterion)\n",
    "    - Inertia (Within-Cluster Sum of Squares)\n",
    "    - Normalized Mutual Information (NMI)\n",
    "    - Cluster Purity\n",
    "#### 17.1 How to compare two different clustering ?\n",
    "- We can use the SSE: SUM of Squared Error \n",
    "- Formula: $SSE={\\sum \\limits _{k=1} ^{K} \\sum \\limits _{y_{i} \\in C_{k}} ||y_{i}-x_{k}||^2}$\n",
    "- $y_{i}$ : is the ith vector belonging to cluster $C_{k}$ and $x_{k}$ is the centroid \n",
    "- Formula of centroid: $$x_{k}={1\\over N_{k}}{\\sum \\limits _{y_{i} \\in C_{k}} y_{i}}$$\n",
    "- If SSE is small, clusters are compact and well separated\n",
    "- Cluster with smallest SSE is the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37601959",
   "metadata": {},
   "source": [
    "### 18 - Hyperparameters tuning or hyperparameter optimization\n",
    "#### 18. 1- What does hyperparameter mean?\n",
    "- Hyperparameters are external configuration settings that are not learned from the data but are set before the training process begins.\n",
    "- These settings influence the learning process and the overall behavior of the model.\n",
    "- Examples of hyperparameters :\n",
    "    - Learning rates\n",
    "    - Regularization parameters\n",
    "    - Hidden layers number\n",
    "    - Nodes number\n",
    "    - Decision tree depth\n",
    "- The choice of hyperparameters, which is called hyperparameter tuning can influence the performance of a machine learning model. \n",
    "- It is crucial to find the optimal values and achieve the best possible predictive performance.\n",
    "\n",
    "#### 18. 2- What does hyperparameter tuning mean? \n",
    "- It is called hyperparameter optimization or model selection.\n",
    "- It corresponds to finding the best set of hyperparameters for a machine learning model.\n",
    "- Here are common steps of Hyperparameter tuning :\n",
    "    - Define a Search Space\n",
    "    - Choose a Search Method\n",
    "    - Choose the right Objective Function\n",
    "    - Search for Optimal Hyperparameters\n",
    "    - Evaluate Performance\n",
    "    - Select Best Hyperparameters\n",
    "    - Final Model Training\n",
    "- **Define a Search Space :** select the set of hyperparameters to be tuned and define a range of possible values for each.\n",
    "- **Choose a Search Method:** choose a Search Method : Grid Search, Random Search, and more advanced techniques like Bayesian optimization.\n",
    "- **Choose the right Objective Function:** select an objective function that evluates the performance of the model for a given set of hyperparameters. Examples: accuracy, precision, recall, or any other relevant measure.\n",
    "- **Select Best Hyperparameters:** it involves training and evaluating the model with various hyperparameter combinations. Then, choose the optimal values.\n",
    "\n",
    "- Hyperparameter tuning is essential for improving the generalization performance of a machine learning model.\n",
    "- It helps to avoid overfitting and ensures that the model is well-configured to handle new, unseen data effectively.\n",
    "\n",
    "#### 18. 3- What is Grid Search? \n",
    "\n",
    "- Gridsearch :\n",
    "    - Performed using `GridSearchCV` of `scikit-learn`.\n",
    "    - It consists on performing an exhaustive search for selecting a model using a predefined hyperparameter grid.\n",
    "    - The data scientist set up a grid of hyperparameters values and for each combination, trains a model and evaluate performance on testing data ==> to select, at the end, the optimal parameters.\n",
    "    - It explores the entire search space by following a grid pattern. \n",
    "    - The search space is defined by specifying discrete values or ranges for each hyperparameter\n",
    "    - It is deep as it guarantees that every combination is evaluated.\n",
    "    - However, it is computationally intensive especially when dealing with a large number of hyperparameters or a broad range of values.\n",
    "    \n",
    "#### 18. 4- What is Random search?\n",
    "    \n",
    "- Randomsearch: \n",
    "    - Set up a grid of hyperparameter values and selects random combinations to train the model and score.  \n",
    "    \n",
    "\n",
    "- Method: Random search randomly samples a specified number of hyperparameter combinations from the defined search space.\n",
    "- Exploration: It explores the hyperparameter space randomly, which can be more efficient in some cases.\n",
    "- Search Space: The search space is defined similarly to grid search but does not require discretization; it can handle continuous and discrete hyperparameters.\n",
    "- Computational Efficiency: Random search is often more computationally efficient than grid search because it does not exhaustively evaluate every combination.\n",
    "    \n",
    "    \n",
    "#### 18. 5- How to choose between Random Search and Grid Search  ?\n",
    "\n",
    "- Comprehensive but Computationally Intensive: Grid search is thorough and guarantees that every combination is evaluated, but it can be computationally intensive, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd02eb",
   "metadata": {},
   "source": [
    "### 19-  What is the difference between a parameter and an hyperparameter? (check)\n",
    "- Each machine learning model has : \n",
    "    - Parameters\n",
    "    - Hyperparameters\n",
    "- **Model parameters:**\n",
    "    - It is a configuration variables that is internal to the model\n",
    "    - It is estimated or learned by the model and not set manually\n",
    "    - It is required to the model to make prediction\n",
    "    - Examples:\n",
    "        - $y=mx+c$ : m and c are parameters\n",
    "        - $y=ax^2+bx+c$ : a, b,c are parameters\n",
    "        - Support vectors in SVM \n",
    "        - Weights in ANN and Linear regression\n",
    "- **Model hyperparameters:**\n",
    "    - They are set before training the model ==> hyperparameters tuning\n",
    "    - They are external to the model \n",
    "    - Can be found using (optimal solution):\n",
    "        - GridSearch \n",
    "        - RandomSearch\n",
    "        - Copy from previous problems\n",
    "    - Or they can be set manually\n",
    "    - Examples:\n",
    "        - Learning rate of NN\n",
    "        - C and *sigma* in SVM\n",
    "        - K in KNN\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526c8be",
   "metadata": {},
   "source": [
    "### 20- What does interpolation and extrapolation mean?\n",
    "- **Interpolation :** \n",
    "    - It is a mathematical and statistical technique used to estimate values that fall between known, observed, or measured data points.\n",
    "    - The goal is to predict values within the range of the existing data\n",
    "- **Extrapolation :**\n",
    "    - Extrapolation comes with more uncertainty compared to interpolation,as it relies on the assumption that the underlying pattern persists outside the known range. \n",
    "    - Extrapolation can be risky, especially when the data may exhibit behavior that deviates from the observed pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21936fcb",
   "metadata": {},
   "source": [
    "### 21- Correlation matrix versus Convariance matrix ? \n",
    "- Correlaion:\n",
    "    - It is is normalized form of covariance.\n",
    "    - It measures the linear relationship of variables.\n",
    "    - Correlation values belongs [-1,1] : negative and positive relations.\n",
    "    - It measures when a change in one variable can result a change in another\n",
    "    - How strongly two random variables are related to each other\n",
    "\n",
    "- Covariance : \n",
    "    - Tells us the direction of the linear relationship between two random variables\n",
    "    - It is used to determine how much two random variables vary together\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e44f8f",
   "metadata": {},
   "source": [
    "### 22- Distributed computing versus parralel computing ? \n",
    "\n",
    "- **Parallel computing:**\n",
    "    - Allows breaking down a large computational task into smaller subtasks that can be executed simultaneously.\n",
    "    - Subtasks execution is done simultaneously in parallel using multiple processors or cores within a single machine.\n",
    "    - Characteristics: Shared Memory, Data Sharing, Single Machine (with multiple processors), lower communication overhead due to accessing shared memory directly. \n",
    "    - Applications : problems that can be divided into independent subtasks such as image processing, numerical simulations, scientific computing.\n",
    "- **Distributed computing:**\n",
    "    - Distributed computing divides a single task between multiple computers (nodes) to achieve a common goal.\n",
    "    - Each computer used in distributed computing has its own processor.\n",
    "    - Machines are often connected over a network, to work together on a computational task\n",
    "    - Characteristics: multiple machines, communication over network, data distribution, designed with fault tolerance mechanisms since individual nodes may fail\n",
    "    - Applications :\n",
    "        - Large-scale data processing (e.g., big data analytics).\n",
    "        - Web services, cloud computing, and distributed databases.\n",
    "        - Solving problems that require the coordination of multiple machines.\n",
    "        \n",
    "- The choice between them depends on the nature of the problem, scale requirements, and communication considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e28de",
   "metadata": {},
   "source": [
    "### 23 What does multicolinearity means?\n",
    "- It is a statistical concept where several independent variables in model are correlated\n",
    "- If correlation coefficient is +/- 1 ==> those two variables \"perfectly collinear\".\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1959654",
   "metadata": {},
   "source": [
    "### What does Bias variance trade off mean?\n",
    "- The bias error is an error from the erroneous assumption in the learning algorithm\n",
    "- High bias ==> underfitting : algorithm missunderstand the relevant relations between features and target outputs.\n",
    "- The variance is an error from sensitivity to small fluctuations in the training set. \n",
    "- High variance==> overfitting : algorithm learns also the noise from the training data \n",
    "\n",
    "![title](images/bias_variance_tradeoff.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2bac0",
   "metadata": {},
   "source": [
    "### What does cardinality mean?\n",
    "- The number of unique values in a column\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbcb741",
   "metadata": {},
   "source": [
    "### 10- What does Instance-Based Learning means : \n",
    "Also known as instance-based reasoning or memory-based learning, is a type of machine learning approach that makes predictions based on the similarity between new instances and instances in the training dataset. Instead of learning an explicit model during training, instance-based learning stores the entire training dataset and uses it to make predictions for new, unseen instances. K-Nearest Neighbors (KNN) is a classic example. \n",
    "\n",
    "It is suited for tasks where the relationships between input features and output labels are not easily captured by a simple model. It can be robust in the presence of noise and is capable of handling complex decision boundaries. However, it may be computationally expensive, especially when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec6543",
   "metadata": {},
   "source": [
    "### Why should we create a ML pipeline: \n",
    "A ML pipeline is an end to end construct that orchestrates the flow of data into and output from a ml model(set of multiple models). \n",
    "It is a way to modify and automate  the workflow it takes to produce a ml model \n",
    "multiple sequential steps from data extraction, preprocessing to model training and deplyment.\n",
    "### What is the difference between Inductive ML and Deductive ML?\n",
    "- **Inductive Learning:**\n",
    "    - Observes instances based on defined principles to draw a conclusion \n",
    "    - Example: explain to child to stay away of the fire and show him video\n",
    "- **Deductive Learning:**\n",
    "    - Conclude experiences \n",
    "    - Example: allow child to play with fire\n",
    "### What is Maximum Information Criterion (MIC)?\n",
    "- It is used to identify relationship between pairs of variables\n",
    "- It measures the strength of linear and non-linear association between two variables x and y\n",
    "- It captures a wide range of associations both functional and non-functional\n",
    "- For functional: it provides $R^2$: coefficient of determination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66d794",
   "metadata": {},
   "source": [
    "### How Logistic regression works ?\n",
    "- It is a classification algorithm used to predict a discret output.\n",
    "- Types of outputs: \n",
    "    - Binary (2 classes)\n",
    "    - Multiple (>2 classes)\n",
    "    - Ordianl (Low, medium, High)\n",
    "- It uses the sigmoid activation function to map predictions to probabilities\n",
    "- Output:mx+b\n",
    "- Sigmoid function formula: $$S(z)={1\\over 1+ e^{-z}}$$\n",
    "<div>\n",
    "<img src=\"images/sigmoid-function.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec870a",
   "metadata": {},
   "source": [
    "### What is 'naive' in the Naive Bayes classifier?\n",
    "- The classifier is called 'naive' because it makes assumptions that may or may not turn out to be correct\n",
    "- The algorithm assumes the absolute independence of features==>the presence of one feature of a class is not related to the presence of any other feature\n",
    "- Example: any fruit that is red and round is cherry ==> it can be true or false\n",
    "### How to knwo which ML algorithm to use for your classification problem ?\n",
    "- There is no fixed rule to choose. However, you can follow these guidelines: \n",
    "    - If accuracy is a concern ==> test different algorithms and cross-validate them\n",
    "    - If the training dataset is small ==> use models that have low varaiance and high bias\n",
    "    - If the training dataset is large ==> use models that have high variance and littke bias\n",
    "### How to choose which ML algorithm tu use given a dataset?\n",
    "- No master algorithm it all depends on the situation\n",
    "- Answer the next questions: \n",
    "    - How much data?\n",
    "    - Output: Continous, Categorical?\n",
    "    - Is it classification, regression or clustering?\n",
    "    - Is all output variables labled or mixed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f5bf5",
   "metadata": {},
   "source": [
    "### What does decision tree means ? \n",
    "- decision tree can be used for :\n",
    "    - Classification\n",
    "    - Regression \n",
    "- We build a tree with datasets broken up into smaller subsets while developing the decision tree\n",
    "- It can handle both categorical and numerical data \n",
    "\n",
    "### What does prunning decision tree means?\n",
    "- Pruning is a technique in ML that reduces the size of DT ==> to reduce the complexity of final classifier \n",
    "- Pruning helps improve the predictive accuracy by reducing overfitting \n",
    "- Pruning can occur in :\n",
    "    - Top-down fashion \n",
    "    - Bottom-down fashion\n",
    "- Top-down fashion : it will traverse nodes and train subsets starting at the root\n",
    "- Bottom-up fashion : it will begin at the leaf nodes \n",
    "### What are the popular pruning algorithms?\n",
    "- Reduced error pruning :\n",
    "    - starts with leaves, each node is replaced with its most popular class\n",
    "    - if the prediction accuracy is not affected the change is kept \n",
    "    - There is an advantage of simplicity and speed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e100dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21752e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a5b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602a04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87050441",
   "metadata": {},
   "source": [
    "- In supervised learning called matching matrix - cm\n",
    "- check logloss in classification report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0caf5e",
   "metadata": {},
   "source": [
    "### What is Bias term ?\n",
    "- represents patterns that do not pass through the origin y=ax+b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d308f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc1871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d3d618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a1e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88886394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c039acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
